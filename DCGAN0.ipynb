{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Setting some constant values for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualSeed = 230\n",
    "datafile = \"./S_configs_64x64.dat\"\n",
    "batch_size = 8\n",
    "num_configs = 800\n",
    "Ny = 64\n",
    "Nx = 64\n",
    "nc = 1\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "num_epochs = 3\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "ngpu = 0\n",
    "N = Ny * Nx\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net\n",
    "#### Generator\n",
    "We follow the standard DCGAN generator network with one difference in the last layer. After tanh in the last layer, we set all elements larger than 0 to 1, and all elements less than 0 to -1, so the output is the usual Ising spin values. Let's call this procedure stepping because the function resembles a step function :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stepping(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stepping, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return (x > 0).type_as(x) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Load data and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(manualSeed)\n",
    "np.random.seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_snapshots.shape = (409600000,)\n"
     ]
    }
   ],
   "source": [
    "S_snapshots = np.fromfile(datafile, dtype=np.int32)\n",
    "print(\"S_snapshots.shape =\", S_snapshots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_snapshots.shape = -1, 1, Ny, Nx\n",
    "# S_snapshots.dtype = np.double\n",
    "# print(\"S_snapshots.shape =\", S_snapshots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(S_snapshots)\n",
    "S_snapshots_tensor = torch.tensor(S_snapshots, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(S_snapshots_tensor[0:num_configs], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.tensor(S_snapshots[:16])\n",
    "b = vutils.make_grid(a, nrow=4, padding=1, normalize=True)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.pcolormesh(b[0], cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Initialize generator and discriminator and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "# GPU\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "G training 1, loss 1.1642659902572632, D(G(z)) 0.35313478112220764\n",
      "G training 2, loss 0.5277465581893921, D(G(z)) 0.6419681906700134\n",
      "G training 3, loss 0.14910630881786346, D(G(z)) 0.8644424080848694\n",
      "G training 4, loss 0.03491305187344551, D(G(z)) 0.966521143913269\n",
      "\n",
      "\n",
      "[0/3][0/100]\tLoss_D: 1.7788\tLoss_G: 0.0349\tD(x): 0.3788\tD(G(z)): 0.4149 / 0.9665\n",
      "g_grad:  tensor(0.0049) d_grad tensor(0.0512)\n",
      "G training 1, loss 0.6381374001502991, D(G(z)) 0.5750578045845032\n",
      "G training 2, loss 0.3020857870578766, D(G(z)) 0.7562564611434937\n",
      "G training 3, loss 0.0631316676735878, D(G(z)) 0.9400709867477417\n",
      "G training 4, loss 0.0774206668138504, D(G(z)) 0.9365833401679993\n",
      "\n",
      "\n",
      "[0/3][1/100]\tLoss_D: 4.8223\tLoss_G: 0.0774\tD(x): 0.5370\tD(G(z)): 0.9686 / 0.9366\n",
      "g_grad:  tensor(-0.0048) d_grad tensor(0.0287)\n",
      "G training 1, loss 1.2997548580169678, D(G(z)) 0.37561675906181335\n",
      "G training 2, loss 0.20828917622566223, D(G(z)) 0.8246215581893921\n",
      "G training 3, loss 0.045699913054704666, D(G(z)) 0.9562090635299683\n",
      "G training 4, loss 0.03332389146089554, D(G(z)) 0.9683812856674194\n",
      "\n",
      "\n",
      "[0/3][2/100]\tLoss_D: 5.6367\tLoss_G: 0.0333\tD(x): 0.4587\tD(G(z)): 0.9815 / 0.9684\n",
      "g_grad:  tensor(-1.2437e-05) d_grad tensor(-0.0533)\n",
      "G training 1, loss 0.8554954528808594, D(G(z)) 0.5069981217384338\n",
      "G training 2, loss 0.1184462383389473, D(G(z)) 0.8952511548995972\n",
      "G training 3, loss 0.022155823186039925, D(G(z)) 0.9782824516296387\n",
      "G training 4, loss 0.048292968422174454, D(G(z)) 0.9548484683036804\n",
      "\n",
      "\n",
      "[0/3][3/100]\tLoss_D: 5.4761\tLoss_G: 0.0483\tD(x): 0.1806\tD(G(z)): 0.9558 / 0.9548\n",
      "g_grad:  tensor(0.0002) d_grad tensor(-0.0280)\n",
      "G training 1, loss 0.6204348206520081, D(G(z)) 0.5810577273368835\n",
      "G training 2, loss 0.2113620936870575, D(G(z)) 0.8192738890647888\n",
      "G training 3, loss 0.04873151332139969, D(G(z)) 0.9529225826263428\n",
      "G training 4, loss 0.047482747584581375, D(G(z)) 0.9553865194320679\n",
      "\n",
      "\n",
      "[0/3][4/100]\tLoss_D: 6.2717\tLoss_G: 0.0475\tD(x): 0.4297\tD(G(z)): 0.9927 / 0.9554\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(0.1270)\n",
      "G training 1, loss 0.8813807964324951, D(G(z)) 0.4578167498111725\n",
      "G training 2, loss 0.20819777250289917, D(G(z)) 0.8224469423294067\n",
      "G training 3, loss 0.03831636905670166, D(G(z)) 0.9626148343086243\n",
      "G training 4, loss 0.10695157945156097, D(G(z)) 0.9169890880584717\n",
      "\n",
      "\n",
      "[0/3][5/100]\tLoss_D: 6.1231\tLoss_G: 0.1070\tD(x): 0.1756\tD(G(z)): 0.9797 / 0.9170\n",
      "g_grad:  tensor(0.0010) d_grad tensor(0.0049)\n",
      "G training 1, loss 0.9990206360816956, D(G(z)) 0.38520365953445435\n",
      "G training 2, loss 0.1281612366437912, D(G(z)) 0.8838720321655273\n",
      "G training 3, loss 0.20062467455863953, D(G(z)) 0.8534278869628906\n",
      "G training 4, loss 0.012903057970106602, D(G(z)) 0.9872548580169678\n",
      "\n",
      "\n",
      "[0/3][6/100]\tLoss_D: 5.3110\tLoss_G: 0.0129\tD(x): 0.1982\tD(G(z)): 0.9022 / 0.9873\n",
      "g_grad:  tensor(-2.2272e-05) d_grad tensor(-0.1400)\n",
      "G training 1, loss 0.3494027554988861, D(G(z)) 0.7285847663879395\n",
      "G training 2, loss 0.10914593189954758, D(G(z)) 0.9099310040473938\n",
      "G training 3, loss 0.01754315197467804, D(G(z)) 0.9826804399490356\n",
      "G training 4, loss 0.014026960358023643, D(G(z)) 0.9862484335899353\n",
      "\n",
      "\n",
      "[0/3][7/100]\tLoss_D: 7.9436\tLoss_G: 0.0140\tD(x): 0.1068\tD(G(z)): 0.9909 / 0.9862\n",
      "g_grad:  tensor(-3.7331e-05) d_grad tensor(-0.0872)\n",
      "G training 1, loss 1.6978721618652344, D(G(z)) 0.34799909591674805\n",
      "G training 2, loss 0.15613339841365814, D(G(z)) 0.8612645864486694\n",
      "G training 3, loss 0.014226581901311874, D(G(z)) 0.9859408736228943\n",
      "G training 4, loss 0.014286963269114494, D(G(z)) 0.9858879446983337\n",
      "\n",
      "\n",
      "[0/3][8/100]\tLoss_D: 7.6194\tLoss_G: 0.0143\tD(x): 0.0859\tD(G(z)): 0.9869 / 0.9859\n",
      "g_grad:  tensor(3.7821e-05) d_grad tensor(-0.1551)\n",
      "G training 1, loss 0.6768743991851807, D(G(z)) 0.544906735420227\n",
      "G training 2, loss 0.13693688809871674, D(G(z)) 0.890231728553772\n",
      "G training 3, loss 0.032304681837558746, D(G(z)) 0.9697941541671753\n",
      "G training 4, loss 0.003856529016047716, D(G(z)) 0.9961569309234619\n",
      "\n",
      "\n",
      "[0/3][9/100]\tLoss_D: 8.8005\tLoss_G: 0.0039\tD(x): 0.1374\tD(G(z)): 0.9914 / 0.9962\n",
      "g_grad:  tensor(4.3646e-06) d_grad tensor(-0.1576)\n",
      "G training 1, loss 1.4204386472702026, D(G(z)) 0.2975729703903198\n",
      "G training 2, loss 0.39951494336128235, D(G(z)) 0.792760968208313\n",
      "G training 3, loss 0.012824103236198425, D(G(z)) 0.9873246550559998\n",
      "G training 4, loss 0.007703927345573902, D(G(z)) 0.9923573136329651\n",
      "\n",
      "\n",
      "[0/3][10/100]\tLoss_D: 7.5860\tLoss_G: 0.0077\tD(x): 0.1852\tD(G(z)): 0.9935 / 0.9924\n",
      "g_grad:  tensor(2.1321e-05) d_grad tensor(-0.0515)\n",
      "G training 1, loss 1.9012216329574585, D(G(z)) 0.21775393187999725\n",
      "G training 2, loss 0.15319736301898956, D(G(z)) 0.8884515762329102\n",
      "G training 3, loss 0.013265361078083515, D(G(z)) 0.9868550896644592\n",
      "G training 4, loss 0.006953002884984016, D(G(z)) 0.9931296110153198\n",
      "\n",
      "\n",
      "[0/3][11/100]\tLoss_D: 8.0772\tLoss_G: 0.0070\tD(x): 0.2052\tD(G(z)): 0.9938 / 0.9931\n",
      "g_grad:  tensor(-1.9993e-05) d_grad tensor(0.0474)\n",
      "G training 1, loss 0.8180769681930542, D(G(z)) 0.5122774839401245\n",
      "G training 2, loss 0.05439525470137596, D(G(z)) 0.9489391446113586\n",
      "G training 3, loss 0.022460689768195152, D(G(z)) 0.9781925082206726\n",
      "G training 4, loss 0.005935450084507465, D(G(z)) 0.9941469430923462\n",
      "\n",
      "\n",
      "[0/3][12/100]\tLoss_D: 9.1911\tLoss_G: 0.0059\tD(x): 0.0756\tD(G(z)): 0.9960 / 0.9941\n",
      "g_grad:  tensor(-2.6924e-05) d_grad tensor(0.0537)\n",
      "G training 1, loss 1.1130928993225098, D(G(z)) 0.4269547164440155\n",
      "G training 2, loss 0.07733383029699326, D(G(z)) 0.9270056486129761\n",
      "G training 3, loss 0.0061776950024068356, D(G(z)) 0.9938561916351318\n",
      "G training 4, loss 0.00802777148783207, D(G(z)) 0.9920337796211243\n",
      "\n",
      "\n",
      "[0/3][13/100]\tLoss_D: 8.2842\tLoss_G: 0.0080\tD(x): 0.1992\tD(G(z)): 0.9965 / 0.9920\n",
      "g_grad:  tensor(1.7614e-05) d_grad tensor(-0.0527)\n",
      "G training 1, loss 1.9795324802398682, D(G(z)) 0.1831648349761963\n",
      "G training 2, loss 0.10053753852844238, D(G(z)) 0.9060076475143433\n",
      "G training 3, loss 0.022130873054265976, D(G(z)) 0.9782375693321228\n",
      "G training 4, loss 0.011322719976305962, D(G(z)) 0.9887816905975342\n",
      "\n",
      "\n",
      "[0/3][14/100]\tLoss_D: 7.5244\tLoss_G: 0.0113\tD(x): 0.2383\tD(G(z)): 0.9955 / 0.9888\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0610)\n",
      "G training 1, loss 2.2029693126678467, D(G(z)) 0.1400819718837738\n",
      "G training 2, loss 0.05209651589393616, D(G(z)) 0.9534476399421692\n",
      "G training 3, loss 0.004801136441528797, D(G(z)) 0.9952161312103271\n",
      "G training 4, loss 0.004707129672169685, D(G(z)) 0.995312511920929\n",
      "\n",
      "\n",
      "[0/3][15/100]\tLoss_D: 9.1268\tLoss_G: 0.0047\tD(x): 0.0824\tD(G(z)): 0.9925 / 0.9953\n",
      "g_grad:  tensor(1.9609e-05) d_grad tensor(0.0249)\n",
      "G training 1, loss 2.21378755569458, D(G(z)) 0.20533382892608643\n",
      "G training 2, loss 0.32493525743484497, D(G(z)) 0.7643013596534729\n",
      "G training 3, loss 0.02477911487221718, D(G(z)) 0.9757601022720337\n",
      "G training 4, loss 0.01595976948738098, D(G(z)) 0.9845843315124512\n",
      "\n",
      "\n",
      "[0/3][16/100]\tLoss_D: 9.9866\tLoss_G: 0.0160\tD(x): 0.0877\tD(G(z)): 0.9973 / 0.9846\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0948)\n",
      "G training 1, loss 1.6776323318481445, D(G(z)) 0.3245919346809387\n",
      "G training 2, loss 0.10216698050498962, D(G(z)) 0.9121022820472717\n",
      "G training 3, loss 0.012469365261495113, D(G(z)) 0.9877089262008667\n",
      "G training 4, loss 0.004268760792911053, D(G(z)) 0.9957461953163147\n",
      "\n",
      "\n",
      "[0/3][17/100]\tLoss_D: 10.0141\tLoss_G: 0.0043\tD(x): 0.1104\tD(G(z)): 0.9991 / 0.9957\n",
      "g_grad:  tensor(1.2990e-05) d_grad tensor(-0.2436)\n",
      "G training 1, loss 1.5824981927871704, D(G(z)) 0.28315266966819763\n",
      "G training 2, loss 0.024320436641573906, D(G(z)) 0.97605961561203\n",
      "G training 3, loss 0.012224074453115463, D(G(z)) 0.9879354238510132\n",
      "G training 4, loss 0.003498914185911417, D(G(z)) 0.996533989906311\n",
      "\n",
      "\n",
      "[0/3][18/100]\tLoss_D: 9.0386\tLoss_G: 0.0035\tD(x): 0.0976\tD(G(z)): 0.9952 / 0.9965\n",
      "g_grad:  tensor(-7.5646e-06) d_grad tensor(-0.0714)\n",
      "G training 1, loss 2.093825340270996, D(G(z)) 0.24013487994670868\n",
      "G training 2, loss 0.22422663867473602, D(G(z)) 0.8135612607002258\n",
      "G training 3, loss 0.009173430502414703, D(G(z)) 0.9908912181854248\n",
      "G training 4, loss 0.004118090961128473, D(G(z)) 0.9958921670913696\n",
      "\n",
      "\n",
      "[0/3][19/100]\tLoss_D: 9.0712\tLoss_G: 0.0041\tD(x): 0.1449\tD(G(z)): 0.9984 / 0.9959\n",
      "g_grad:  tensor(-5.3947e-05) d_grad tensor(-0.3006)\n",
      "G training 1, loss 4.249833106994629, D(G(z)) 0.03810499235987663\n",
      "G training 2, loss 0.6637970209121704, D(G(z)) 0.6554093956947327\n",
      "G training 3, loss 0.03744729608297348, D(G(z)) 0.9634667634963989\n",
      "G training 4, loss 0.020343508571386337, D(G(z)) 0.980061411857605\n",
      "\n",
      "\n",
      "[0/3][20/100]\tLoss_D: 8.5905\tLoss_G: 0.0203\tD(x): 0.0934\tD(G(z)): 0.9971 / 0.9801\n",
      "g_grad:  tensor(0.0004) d_grad tensor(-0.2646)\n",
      "G training 1, loss 4.688019275665283, D(G(z)) 0.013031607493758202\n",
      "G training 2, loss 0.6309573650360107, D(G(z)) 0.541771650314331\n",
      "G training 3, loss 0.09043164551258087, D(G(z)) 0.9245404005050659\n",
      "G training 4, loss 0.013034644536674023, D(G(z)) 0.9871375560760498\n",
      "\n",
      "\n",
      "[0/3][21/100]\tLoss_D: 7.7576\tLoss_G: 0.0130\tD(x): 0.1208\tD(G(z)): 0.9950 / 0.9871\n",
      "g_grad:  tensor(-4.7216e-05) d_grad tensor(-0.3720)\n",
      "G training 1, loss 3.0723588466644287, D(G(z)) 0.07973597943782806\n",
      "G training 2, loss 0.23411723971366882, D(G(z)) 0.8046383857727051\n",
      "G training 3, loss 0.016052182763814926, D(G(z)) 0.984213650226593\n",
      "G training 4, loss 0.0068945735692977905, D(G(z)) 0.9931612610816956\n",
      "\n",
      "\n",
      "[0/3][22/100]\tLoss_D: 8.9174\tLoss_G: 0.0069\tD(x): 0.0354\tD(G(z)): 0.9889 / 0.9932\n",
      "g_grad:  tensor(-1.6713e-05) d_grad tensor(0.1269)\n",
      "G training 1, loss 3.669780969619751, D(G(z)) 0.034229401499032974\n",
      "G training 2, loss 0.34785592555999756, D(G(z)) 0.7482834458351135\n",
      "G training 3, loss 0.012616237625479698, D(G(z)) 0.9875302314758301\n",
      "G training 4, loss 0.00581081910058856, D(G(z)) 0.9942175149917603\n",
      "\n",
      "\n",
      "[0/3][23/100]\tLoss_D: 7.9654\tLoss_G: 0.0058\tD(x): 0.1177\tD(G(z)): 0.9926 / 0.9942\n",
      "g_grad:  tensor(-3.5640e-06) d_grad tensor(0.1297)\n",
      "G training 1, loss 3.515794515609741, D(G(z)) 0.035139188170433044\n",
      "G training 2, loss 0.4283880889415741, D(G(z)) 0.6811926364898682\n",
      "G training 3, loss 0.11963684856891632, D(G(z)) 0.8987374305725098\n",
      "G training 4, loss 0.014141611754894257, D(G(z)) 0.9859729409217834\n",
      "\n",
      "\n",
      "[0/3][24/100]\tLoss_D: 7.4770\tLoss_G: 0.0141\tD(x): 0.2205\tD(G(z)): 0.9935 / 0.9860\n",
      "g_grad:  tensor(6.5271e-05) d_grad tensor(-0.1058)\n",
      "G training 1, loss 4.722225189208984, D(G(z)) 0.014518620446324348\n",
      "G training 2, loss 0.73939049243927, D(G(z)) 0.550607442855835\n",
      "G training 3, loss 0.05163394659757614, D(G(z)) 0.9511188864707947\n",
      "G training 4, loss 0.02278190292418003, D(G(z)) 0.9776855111122131\n",
      "\n",
      "\n",
      "[0/3][25/100]\tLoss_D: 7.4255\tLoss_G: 0.0228\tD(x): 0.1491\tD(G(z)): 0.9769 / 0.9777\n",
      "g_grad:  tensor(-2.4992e-05) d_grad tensor(-0.1846)\n",
      "G training 1, loss 4.554646968841553, D(G(z)) 0.012151955626904964\n",
      "G training 2, loss 0.6623207926750183, D(G(z)) 0.562658429145813\n",
      "G training 3, loss 0.0700579360127449, D(G(z)) 0.9340227842330933\n",
      "G training 4, loss 0.016661979258060455, D(G(z)) 0.9835474491119385\n",
      "\n",
      "\n",
      "[0/3][26/100]\tLoss_D: 7.9754\tLoss_G: 0.0167\tD(x): 0.0993\tD(G(z)): 0.9901 / 0.9835\n",
      "g_grad:  tensor(5.0321e-05) d_grad tensor(-0.1386)\n",
      "G training 1, loss 4.381071090698242, D(G(z)) 0.016024738550186157\n",
      "G training 2, loss 0.40336617827415466, D(G(z)) 0.6823385953903198\n",
      "G training 3, loss 0.05259999260306358, D(G(z)) 0.9501506090164185\n",
      "G training 4, loss 0.01837446168065071, D(G(z)) 0.9819666147232056\n",
      "\n",
      "\n",
      "[0/3][27/100]\tLoss_D: 9.8912\tLoss_G: 0.0184\tD(x): 0.0134\tD(G(z)): 0.9840 / 0.9820\n",
      "g_grad:  tensor(0.0001) d_grad tensor(-0.1060)\n",
      "G training 1, loss 3.6657307147979736, D(G(z)) 0.04148302599787712\n",
      "G training 2, loss 0.11703948676586151, D(G(z)) 0.8937244415283203\n",
      "G training 3, loss 0.027282819151878357, D(G(z)) 0.9734032154083252\n",
      "G training 4, loss 0.010016010142862797, D(G(z)) 0.99007248878479\n",
      "\n",
      "\n",
      "[0/3][28/100]\tLoss_D: 8.1306\tLoss_G: 0.0100\tD(x): 0.1524\tD(G(z)): 0.9962 / 0.9901\n",
      "g_grad:  tensor(6.9980e-06) d_grad tensor(0.1793)\n",
      "G training 1, loss 4.88150691986084, D(G(z)) 0.010014787316322327\n",
      "G training 2, loss 1.166964054107666, D(G(z)) 0.36371248960494995\n",
      "G training 3, loss 0.05183270573616028, D(G(z)) 0.9497671127319336\n",
      "G training 4, loss 0.016218828037381172, D(G(z)) 0.9839478135108948\n",
      "\n",
      "\n",
      "[0/3][29/100]\tLoss_D: 7.1982\tLoss_G: 0.0162\tD(x): 0.2198\tD(G(z)): 0.9925 / 0.9839\n",
      "g_grad:  tensor(7.5765e-05) d_grad tensor(0.6748)\n",
      "G training 1, loss 5.430947303771973, D(G(z)) 0.006604494992643595\n",
      "G training 2, loss 1.3269535303115845, D(G(z)) 0.31294554471969604\n",
      "G training 3, loss 0.09662788361310959, D(G(z)) 0.9090646505355835\n",
      "G training 4, loss 0.03070317581295967, D(G(z)) 0.9703575968742371\n",
      "\n",
      "\n",
      "[0/3][30/100]\tLoss_D: 8.0275\tLoss_G: 0.0307\tD(x): 0.0487\tD(G(z)): 0.9862 / 0.9704\n",
      "g_grad:  tensor(-5.1515e-05) d_grad tensor(0.2985)\n",
      "G training 1, loss 4.806876182556152, D(G(z)) 0.012179398909211159\n",
      "G training 2, loss 1.240172028541565, D(G(z)) 0.3422360122203827\n",
      "G training 3, loss 0.1480119675397873, D(G(z)) 0.8663222789764404\n",
      "G training 4, loss 0.05290735512971878, D(G(z)) 0.9485938549041748\n",
      "\n",
      "\n",
      "[0/3][31/100]\tLoss_D: 10.4551\tLoss_G: 0.0529\tD(x): 0.0350\tD(G(z)): 0.9924 / 0.9486\n",
      "g_grad:  tensor(-5.1003e-05) d_grad tensor(0.0490)\n",
      "G training 1, loss 5.393404006958008, D(G(z)) 0.00872728694230318\n",
      "G training 2, loss 2.240147113800049, D(G(z)) 0.13316909968852997\n",
      "G training 3, loss 0.16787651181221008, D(G(z)) 0.8538520336151123\n",
      "G training 4, loss 0.031648341566324234, D(G(z)) 0.9690761566162109\n",
      "\n",
      "\n",
      "[0/3][32/100]\tLoss_D: 5.9800\tLoss_G: 0.0316\tD(x): 0.1253\tD(G(z)): 0.9607 / 0.9691\n",
      "g_grad:  tensor(-7.8007e-05) d_grad tensor(-0.2193)\n",
      "G training 1, loss 3.074949264526367, D(G(z)) 0.05969410389661789\n",
      "G training 2, loss 0.5876204967498779, D(G(z)) 0.5874278545379639\n",
      "G training 3, loss 0.043601177632808685, D(G(z)) 0.9577789306640625\n",
      "G training 4, loss 0.023038208484649658, D(G(z)) 0.9775429368019104\n",
      "\n",
      "\n",
      "[0/3][33/100]\tLoss_D: 6.6717\tLoss_G: 0.0230\tD(x): 0.1529\tD(G(z)): 0.9846 / 0.9775\n",
      "g_grad:  tensor(1.6764e-05) d_grad tensor(0.1805)\n",
      "G training 1, loss 2.9421820640563965, D(G(z)) 0.07608567923307419\n",
      "G training 2, loss 0.34909573197364807, D(G(z)) 0.7926028370857239\n",
      "G training 3, loss 0.035600341856479645, D(G(z)) 0.9652454257011414\n",
      "G training 4, loss 0.005395511165261269, D(G(z)) 0.9946224689483643\n",
      "\n",
      "\n",
      "[0/3][34/100]\tLoss_D: 7.9172\tLoss_G: 0.0054\tD(x): 0.0615\tD(G(z)): 0.9886 / 0.9946\n",
      "g_grad:  tensor(3.1282e-06) d_grad tensor(0.0562)\n",
      "G training 1, loss 2.784146547317505, D(G(z)) 0.08105430752038956\n",
      "G training 2, loss 0.30271968245506287, D(G(z)) 0.7477223873138428\n",
      "G training 3, loss 0.037319496273994446, D(G(z)) 0.9635452628135681\n",
      "G training 4, loss 0.025496361777186394, D(G(z)) 0.9750989079475403\n",
      "\n",
      "\n",
      "[0/3][35/100]\tLoss_D: 7.2069\tLoss_G: 0.0255\tD(x): 0.1604\tD(G(z)): 0.9819 / 0.9751\n",
      "g_grad:  tensor(1.3384e-05) d_grad tensor(0.1339)\n",
      "G training 1, loss 4.236665725708008, D(G(z)) 0.016344726085662842\n",
      "G training 2, loss 1.3161728382110596, D(G(z)) 0.2910299599170685\n",
      "G training 3, loss 0.11028797179460526, D(G(z)) 0.8982141017913818\n",
      "G training 4, loss 0.0683303028345108, D(G(z)) 0.9354824423789978\n",
      "\n",
      "\n",
      "[0/3][36/100]\tLoss_D: 7.5714\tLoss_G: 0.0683\tD(x): 0.1210\tD(G(z)): 0.9931 / 0.9355\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(0.0044)\n",
      "G training 1, loss 5.904262065887451, D(G(z)) 0.0031257853843271732\n",
      "G training 2, loss 3.400071144104004, D(G(z)) 0.04439317062497139\n",
      "G training 3, loss 0.9956763982772827, D(G(z)) 0.39222660660743713\n",
      "G training 4, loss 0.2252240777015686, D(G(z)) 0.8034212589263916\n",
      "\n",
      "\n",
      "[0/3][37/100]\tLoss_D: 6.0538\tLoss_G: 0.2252\tD(x): 0.1463\tD(G(z)): 0.9798 / 0.8034\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0042)\n",
      "G training 1, loss 4.798097610473633, D(G(z)) 0.01015043817460537\n",
      "G training 2, loss 1.536952257156372, D(G(z)) 0.2328934371471405\n",
      "G training 3, loss 0.17330023646354675, D(G(z)) 0.8435238599777222\n",
      "G training 4, loss 0.07362493127584457, D(G(z)) 0.9311338067054749\n",
      "\n",
      "\n",
      "[0/3][38/100]\tLoss_D: 8.5398\tLoss_G: 0.0736\tD(x): 0.0066\tD(G(z)): 0.9034 / 0.9311\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0037)\n",
      "G training 1, loss 3.7711806297302246, D(G(z)) 0.027514973655343056\n",
      "G training 2, loss 0.7537744045257568, D(G(z)) 0.4854278862476349\n",
      "G training 3, loss 0.12325869500637054, D(G(z)) 0.885920524597168\n",
      "G training 4, loss 0.033007264137268066, D(G(z)) 0.9676122665405273\n",
      "\n",
      "\n",
      "[0/3][39/100]\tLoss_D: 6.7113\tLoss_G: 0.0330\tD(x): 0.1007\tD(G(z)): 0.9794 / 0.9676\n",
      "g_grad:  tensor(1.5979e-05) d_grad tensor(0.0607)\n",
      "G training 1, loss 2.6156344413757324, D(G(z)) 0.09548672288656235\n",
      "G training 2, loss 0.6210048794746399, D(G(z)) 0.581885039806366\n",
      "G training 3, loss 0.15168167650699615, D(G(z)) 0.8616219758987427\n",
      "G training 4, loss 0.06653579324483871, D(G(z)) 0.9360904693603516\n",
      "\n",
      "\n",
      "[0/3][40/100]\tLoss_D: 6.1324\tLoss_G: 0.0665\tD(x): 0.1414\tD(G(z)): 0.9784 / 0.9361\n",
      "g_grad:  tensor(-0.0004) d_grad tensor(-0.0402)\n",
      "G training 1, loss 3.1103522777557373, D(G(z)) 0.05753903463482857\n",
      "G training 2, loss 1.2353031635284424, D(G(z)) 0.3003862798213959\n",
      "G training 3, loss 0.2321368157863617, D(G(z)) 0.7984834909439087\n",
      "G training 4, loss 0.07454830408096313, D(G(z)) 0.9294452667236328\n",
      "\n",
      "\n",
      "[0/3][41/100]\tLoss_D: 7.2682\tLoss_G: 0.0745\tD(x): 0.0396\tD(G(z)): 0.9737 / 0.9294\n",
      "g_grad:  tensor(5.0529e-05) d_grad tensor(0.0929)\n",
      "G training 1, loss 3.1455204486846924, D(G(z)) 0.0510701946914196\n",
      "G training 2, loss 0.7868536114692688, D(G(z)) 0.49444514513015747\n",
      "G training 3, loss 0.16827835142612457, D(G(z)) 0.8459110856056213\n",
      "G training 4, loss 0.10138679295778275, D(G(z)) 0.9042062163352966\n",
      "\n",
      "\n",
      "[0/3][42/100]\tLoss_D: 6.1391\tLoss_G: 0.1014\tD(x): 0.1397\tD(G(z)): 0.9322 / 0.9042\n",
      "g_grad:  tensor(0.0009) d_grad tensor(0.0200)\n",
      "G training 1, loss 3.588252305984497, D(G(z)) 0.03210887685418129\n",
      "G training 2, loss 2.0819289684295654, D(G(z)) 0.1501479297876358\n",
      "G training 3, loss 0.7276302576065063, D(G(z)) 0.516473114490509\n",
      "G training 4, loss 0.2076178789138794, D(G(z)) 0.8132010698318481\n",
      "\n",
      "\n",
      "[0/3][43/100]\tLoss_D: 5.1694\tLoss_G: 0.2076\tD(x): 0.1258\tD(G(z)): 0.9322 / 0.8132\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(-0.0852)\n",
      "G training 1, loss 3.7259316444396973, D(G(z)) 0.034556590020656586\n",
      "G training 2, loss 1.542602300643921, D(G(z)) 0.25034835934638977\n",
      "G training 3, loss 0.3813910484313965, D(G(z)) 0.7008887529373169\n",
      "G training 4, loss 0.11207766830921173, D(G(z)) 0.8959604501724243\n",
      "\n",
      "\n",
      "[0/3][44/100]\tLoss_D: 5.2496\tLoss_G: 0.1121\tD(x): 0.1045\tD(G(z)): 0.9226 / 0.8960\n",
      "g_grad:  tensor(-0.0004) d_grad tensor(0.0014)\n",
      "G training 1, loss 2.7166805267333984, D(G(z)) 0.08088522404432297\n",
      "G training 2, loss 0.5366036295890808, D(G(z)) 0.5986528992652893\n",
      "G training 3, loss 0.4072014093399048, D(G(z)) 0.6779895424842834\n",
      "G training 4, loss 0.11103897541761398, D(G(z)) 0.8978766798973083\n",
      "\n",
      "\n",
      "[0/3][45/100]\tLoss_D: 4.3414\tLoss_G: 0.1110\tD(x): 0.1877\tD(G(z)): 0.8810 / 0.8979\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.1609)\n",
      "G training 1, loss 1.1483315229415894, D(G(z)) 0.35882747173309326\n",
      "G training 2, loss 0.5913838148117065, D(G(z)) 0.5697956085205078\n",
      "G training 3, loss 0.2995130717754364, D(G(z)) 0.7468826174736023\n",
      "G training 4, loss 0.32699552178382874, D(G(z)) 0.7465613484382629\n",
      "\n",
      "\n",
      "[0/3][46/100]\tLoss_D: 5.0665\tLoss_G: 0.3270\tD(x): 0.2151\tD(G(z)): 0.8815 / 0.7466\n",
      "g_grad:  tensor(-0.0044) d_grad tensor(0.0602)\n",
      "G training 1, loss 0.9360473155975342, D(G(z)) 0.40185949206352234\n",
      "G training 2, loss 0.6965567469596863, D(G(z)) 0.5730462074279785\n",
      "G training 3, loss 0.6239766478538513, D(G(z)) 0.6319054365158081\n",
      "G training 4, loss 0.37947970628738403, D(G(z)) 0.711123526096344\n",
      "\n",
      "\n",
      "[0/3][47/100]\tLoss_D: 6.0543\tLoss_G: 0.3795\tD(x): 0.0378\tD(G(z)): 0.8808 / 0.7111\n",
      "g_grad:  tensor(0.0034) d_grad tensor(0.0088)\n",
      "G training 1, loss 1.0320088863372803, D(G(z)) 0.4184233248233795\n",
      "G training 2, loss 1.0873228311538696, D(G(z)) 0.38270753622055054\n",
      "G training 3, loss 0.24848750233650208, D(G(z)) 0.7886701822280884\n",
      "G training 4, loss 0.4050602316856384, D(G(z)) 0.7158814668655396\n",
      "\n",
      "\n",
      "[0/3][48/100]\tLoss_D: 5.2788\tLoss_G: 0.4051\tD(x): 0.1508\tD(G(z)): 0.8930 / 0.7159\n",
      "g_grad:  tensor(0.0018) d_grad tensor(0.0417)\n",
      "G training 1, loss 2.1592323780059814, D(G(z)) 0.1563713401556015\n",
      "G training 2, loss 0.6888124942779541, D(G(z)) 0.526975154876709\n",
      "G training 3, loss 0.4506828188896179, D(G(z)) 0.6745472550392151\n",
      "G training 4, loss 0.3616785407066345, D(G(z)) 0.7165620923042297\n",
      "\n",
      "\n",
      "[0/3][49/100]\tLoss_D: 4.7381\tLoss_G: 0.3617\tD(x): 0.1110\tD(G(z)): 0.8245 / 0.7166\n",
      "g_grad:  tensor(-0.0019) d_grad tensor(-0.0186)\n",
      "G training 1, loss 1.781906247138977, D(G(z)) 0.19153907895088196\n",
      "G training 2, loss 0.9225896000862122, D(G(z)) 0.45804184675216675\n",
      "G training 3, loss 0.34021836519241333, D(G(z)) 0.7520753145217896\n",
      "G training 4, loss 0.3707352876663208, D(G(z)) 0.7035893201828003\n",
      "\n",
      "\n",
      "[0/3][50/100]\tLoss_D: 2.6485\tLoss_G: 0.3707\tD(x): 0.2698\tD(G(z)): 0.6875 / 0.7036\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0054)\n",
      "G training 1, loss 1.2285349369049072, D(G(z)) 0.34241390228271484\n",
      "G training 2, loss 0.6199671030044556, D(G(z)) 0.5933948755264282\n",
      "G training 3, loss 0.6915431022644043, D(G(z)) 0.5245449542999268\n",
      "G training 4, loss 0.24764207005500793, D(G(z)) 0.7889777421951294\n",
      "\n",
      "\n",
      "[0/3][51/100]\tLoss_D: 3.9443\tLoss_G: 0.2476\tD(x): 0.2203\tD(G(z)): 0.8369 / 0.7890\n",
      "g_grad:  tensor(-0.0010) d_grad tensor(0.0125)\n",
      "G training 1, loss 0.7572178840637207, D(G(z)) 0.498918354511261\n",
      "G training 2, loss 0.4006384015083313, D(G(z)) 0.682306706905365\n",
      "G training 3, loss 0.36549052596092224, D(G(z)) 0.6996298432350159\n",
      "G training 4, loss 0.8443914651870728, D(G(z)) 0.4625204801559448\n",
      "\n",
      "\n",
      "[0/3][52/100]\tLoss_D: 3.6380\tLoss_G: 0.8444\tD(x): 0.2286\tD(G(z)): 0.8362 / 0.4625\n",
      "g_grad:  tensor(0.0064) d_grad tensor(-0.0384)\n",
      "G training 1, loss 0.5706647634506226, D(G(z)) 0.5788723230361938\n",
      "G training 2, loss 0.7406263947486877, D(G(z)) 0.510335385799408\n",
      "G training 3, loss 0.6652016639709473, D(G(z)) 0.5275941491127014\n",
      "G training 4, loss 0.36369770765304565, D(G(z)) 0.7078653573989868\n",
      "\n",
      "\n",
      "[0/3][53/100]\tLoss_D: 3.4709\tLoss_G: 0.3637\tD(x): 0.1630\tD(G(z)): 0.7401 / 0.7079\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0016)\n",
      "G training 1, loss 0.6288560628890991, D(G(z)) 0.5576184988021851\n",
      "G training 2, loss 0.4547537863254547, D(G(z)) 0.641258955001831\n",
      "G training 3, loss 0.4323561489582062, D(G(z)) 0.6706122159957886\n",
      "G training 4, loss 0.3069838583469391, D(G(z)) 0.7427173256874084\n",
      "\n",
      "\n",
      "[0/3][54/100]\tLoss_D: 3.1321\tLoss_G: 0.3070\tD(x): 0.2138\tD(G(z)): 0.7274 / 0.7427\n",
      "g_grad:  tensor(-0.0015) d_grad tensor(0.0334)\n",
      "G training 1, loss 1.6128053665161133, D(G(z)) 0.22959528863430023\n",
      "G training 2, loss 0.7408488988876343, D(G(z)) 0.508956789970398\n",
      "G training 3, loss 0.43270015716552734, D(G(z)) 0.6514344215393066\n",
      "G training 4, loss 0.36275994777679443, D(G(z)) 0.702904999256134\n",
      "\n",
      "\n",
      "[0/3][55/100]\tLoss_D: 3.3271\tLoss_G: 0.3628\tD(x): 0.1892\tD(G(z)): 0.6602 / 0.7029\n",
      "g_grad:  tensor(0.0021) d_grad tensor(0.0728)\n",
      "G training 1, loss 0.8915016055107117, D(G(z)) 0.42248231172561646\n",
      "G training 2, loss 0.9782434105873108, D(G(z)) 0.3993939459323883\n",
      "G training 3, loss 0.6957829594612122, D(G(z)) 0.5175075531005859\n",
      "G training 4, loss 0.431441068649292, D(G(z)) 0.6531442999839783\n",
      "\n",
      "\n",
      "[0/3][56/100]\tLoss_D: 4.1192\tLoss_G: 0.4314\tD(x): 0.1231\tD(G(z)): 0.7752 / 0.6531\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0170)\n",
      "G training 1, loss 1.1079355478286743, D(G(z)) 0.3419540226459503\n",
      "G training 2, loss 0.5973324775695801, D(G(z)) 0.55643630027771\n",
      "G training 3, loss 1.0833790302276611, D(G(z)) 0.3580039143562317\n",
      "G training 4, loss 0.6220422387123108, D(G(z)) 0.5458613634109497\n",
      "\n",
      "\n",
      "[0/3][57/100]\tLoss_D: 2.7554\tLoss_G: 0.6220\tD(x): 0.3635\tD(G(z)): 0.7925 / 0.5459\n",
      "g_grad:  tensor(-1.1447e-05) d_grad tensor(4.9575e-05)\n",
      "G training 1, loss 0.9164474606513977, D(G(z)) 0.42206764221191406\n",
      "G training 2, loss 0.8135061264038086, D(G(z)) 0.4695766866207123\n",
      "G training 3, loss 0.7924296259880066, D(G(z)) 0.5129210948944092\n",
      "G training 4, loss 0.46707627177238464, D(G(z)) 0.6368045806884766\n",
      "\n",
      "\n",
      "[0/3][58/100]\tLoss_D: 5.1591\tLoss_G: 0.4671\tD(x): 0.0399\tD(G(z)): 0.7331 / 0.6368\n",
      "g_grad:  tensor(0.0009) d_grad tensor(-0.0079)\n",
      "G training 1, loss 0.6314634680747986, D(G(z)) 0.542741060256958\n",
      "G training 2, loss 0.5262999534606934, D(G(z)) 0.6026787161827087\n",
      "G training 3, loss 0.5257370471954346, D(G(z)) 0.6041573882102966\n",
      "G training 4, loss 0.2733617126941681, D(G(z)) 0.7697575092315674\n",
      "\n",
      "\n",
      "[0/3][59/100]\tLoss_D: 4.6845\tLoss_G: 0.2734\tD(x): 0.0373\tD(G(z)): 0.5871 / 0.7698\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(-0.0250)\n",
      "G training 1, loss 0.9929968118667603, D(G(z)) 0.3891063630580902\n",
      "G training 2, loss 0.7806240916252136, D(G(z)) 0.4623689651489258\n",
      "G training 3, loss 0.5045369863510132, D(G(z)) 0.6114572286605835\n",
      "G training 4, loss 0.36863917112350464, D(G(z)) 0.6958370804786682\n",
      "\n",
      "\n",
      "[0/3][60/100]\tLoss_D: 3.1741\tLoss_G: 0.3686\tD(x): 0.2438\tD(G(z)): 0.7530 / 0.6958\n",
      "g_grad:  tensor(-9.8443e-05) d_grad tensor(-0.0159)\n",
      "G training 1, loss 0.6782633066177368, D(G(z)) 0.5201096534729004\n",
      "G training 2, loss 0.6705789566040039, D(G(z)) 0.5217427611351013\n",
      "G training 3, loss 0.46156203746795654, D(G(z)) 0.6331113576889038\n",
      "G training 4, loss 0.38033026456832886, D(G(z)) 0.685420572757721\n",
      "\n",
      "\n",
      "[0/3][61/100]\tLoss_D: 2.0608\tLoss_G: 0.3803\tD(x): 0.4131\tD(G(z)): 0.6492 / 0.6854\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0022)\n",
      "G training 1, loss 0.587329089641571, D(G(z)) 0.5604203343391418\n",
      "G training 2, loss 0.9562813639640808, D(G(z)) 0.4208860397338867\n",
      "G training 3, loss 0.3968304991722107, D(G(z)) 0.6738905906677246\n",
      "G training 4, loss 0.42855969071388245, D(G(z)) 0.6550815105438232\n",
      "\n",
      "\n",
      "[0/3][62/100]\tLoss_D: 3.0921\tLoss_G: 0.4286\tD(x): 0.1917\tD(G(z)): 0.6237 / 0.6551\n",
      "g_grad:  tensor(0.0010) d_grad tensor(0.0179)\n",
      "G training 1, loss 0.5987122058868408, D(G(z)) 0.5737564563751221\n",
      "G training 2, loss 0.782450795173645, D(G(z)) 0.467967689037323\n",
      "G training 3, loss 0.5237715840339661, D(G(z)) 0.6035938858985901\n",
      "G training 4, loss 0.4703391492366791, D(G(z)) 0.6300779581069946\n",
      "\n",
      "\n",
      "[0/3][63/100]\tLoss_D: 2.0991\tLoss_G: 0.4703\tD(x): 0.4417\tD(G(z)): 0.6986 / 0.6301\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(0.0272)\n",
      "G training 1, loss 0.6353610754013062, D(G(z)) 0.5354084968566895\n",
      "G training 2, loss 0.8574065566062927, D(G(z)) 0.4328625798225403\n",
      "G training 3, loss 0.6086249947547913, D(G(z)) 0.5611896514892578\n",
      "G training 4, loss 0.500868022441864, D(G(z)) 0.6119176149368286\n",
      "\n",
      "\n",
      "[0/3][64/100]\tLoss_D: 2.9215\tLoss_G: 0.5009\tD(x): 0.2280\tD(G(z)): 0.6705 / 0.6119\n",
      "g_grad:  tensor(0.0014) d_grad tensor(0.0522)\n",
      "G training 1, loss 0.59210604429245, D(G(z)) 0.5645294189453125\n",
      "G training 2, loss 0.5222786068916321, D(G(z)) 0.6056040525436401\n",
      "G training 3, loss 0.42992421984672546, D(G(z)) 0.6612058281898499\n",
      "G training 4, loss 0.44533947110176086, D(G(z)) 0.6531590819358826\n",
      "\n",
      "\n",
      "[0/3][65/100]\tLoss_D: 5.0996\tLoss_G: 0.4453\tD(x): 0.0501\tD(G(z)): 0.7185 / 0.6532\n",
      "g_grad:  tensor(-2.4824e-05) d_grad tensor(0.0116)\n",
      "G training 1, loss 0.6930450797080994, D(G(z)) 0.5217421650886536\n",
      "G training 2, loss 0.49401283264160156, D(G(z)) 0.6183803081512451\n",
      "G training 3, loss 0.5442857146263123, D(G(z)) 0.6032292246818542\n",
      "G training 4, loss 0.38471198081970215, D(G(z)) 0.68824702501297\n",
      "\n",
      "\n",
      "[0/3][66/100]\tLoss_D: 6.0131\tLoss_G: 0.3847\tD(x): 0.0224\tD(G(z)): 0.6840 / 0.6882\n",
      "g_grad:  tensor(-0.0009) d_grad tensor(0.0040)\n",
      "G training 1, loss 0.5957461595535278, D(G(z)) 0.5536154508590698\n",
      "G training 2, loss 0.6021573543548584, D(G(z)) 0.5657368898391724\n",
      "G training 3, loss 0.5714271068572998, D(G(z)) 0.571273922920227\n",
      "G training 4, loss 0.3975101113319397, D(G(z)) 0.674371063709259\n",
      "\n",
      "\n",
      "[0/3][67/100]\tLoss_D: 4.4965\tLoss_G: 0.3975\tD(x): 0.0626\tD(G(z)): 0.6819 / 0.6744\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(0.0575)\n",
      "G training 1, loss 0.6899044513702393, D(G(z)) 0.5078661441802979\n",
      "G training 2, loss 0.48591500520706177, D(G(z)) 0.6245635151863098\n",
      "G training 3, loss 0.38499996066093445, D(G(z)) 0.6830260753631592\n",
      "G training 4, loss 0.440762460231781, D(G(z)) 0.6579506397247314\n",
      "\n",
      "\n",
      "[0/3][68/100]\tLoss_D: 3.8744\tLoss_G: 0.4408\tD(x): 0.1948\tD(G(z)): 0.7400 / 0.6580\n",
      "g_grad:  tensor(-0.0028) d_grad tensor(-0.0158)\n",
      "G training 1, loss 0.6778388023376465, D(G(z)) 0.5127674341201782\n",
      "G training 2, loss 0.42109835147857666, D(G(z)) 0.6620967984199524\n",
      "G training 3, loss 0.5594822764396667, D(G(z)) 0.5760288238525391\n",
      "G training 4, loss 0.3784608244895935, D(G(z)) 0.6932666301727295\n",
      "\n",
      "\n",
      "[0/3][69/100]\tLoss_D: 2.2222\tLoss_G: 0.3785\tD(x): 0.5350\tD(G(z)): 0.7874 / 0.6933\n",
      "g_grad:  tensor(-3.5438e-05) d_grad tensor(-0.0047)\n",
      "G training 1, loss 0.5169872045516968, D(G(z)) 0.6122792959213257\n",
      "G training 2, loss 0.40876492857933044, D(G(z)) 0.665399432182312\n",
      "G training 3, loss 0.7059456706047058, D(G(z)) 0.5026812553405762\n",
      "G training 4, loss 0.3487203121185303, D(G(z)) 0.7129402756690979\n",
      "\n",
      "\n",
      "[0/3][70/100]\tLoss_D: 3.0468\tLoss_G: 0.3487\tD(x): 0.3201\tD(G(z)): 0.8126 / 0.7129\n",
      "g_grad:  tensor(0.0013) d_grad tensor(-0.0138)\n",
      "G training 1, loss 0.9091111421585083, D(G(z)) 0.4587283134460449\n",
      "G training 2, loss 0.7425039410591125, D(G(z)) 0.5230065584182739\n",
      "G training 3, loss 0.7313348650932312, D(G(z)) 0.5402523875236511\n",
      "G training 4, loss 0.5654206275939941, D(G(z)) 0.5752366781234741\n",
      "\n",
      "\n",
      "[0/3][71/100]\tLoss_D: 2.3372\tLoss_G: 0.5654\tD(x): 0.3968\tD(G(z)): 0.7198 / 0.5752\n",
      "g_grad:  tensor(-0.0015) d_grad tensor(-0.0385)\n",
      "G training 1, loss 0.9371923208236694, D(G(z)) 0.44241881370544434\n",
      "G training 2, loss 0.8384788632392883, D(G(z)) 0.4614567756652832\n",
      "G training 3, loss 0.7187343835830688, D(G(z)) 0.5628355741500854\n",
      "G training 4, loss 0.5035353899002075, D(G(z)) 0.6152088642120361\n",
      "\n",
      "\n",
      "[0/3][72/100]\tLoss_D: 1.8362\tLoss_G: 0.5035\tD(x): 0.4744\tD(G(z)): 0.6196 / 0.6152\n",
      "g_grad:  tensor(0.0034) d_grad tensor(-0.0381)\n",
      "G training 1, loss 0.5901358723640442, D(G(z)) 0.5565243363380432\n",
      "G training 2, loss 0.8604394197463989, D(G(z)) 0.5244702100753784\n",
      "G training 3, loss 0.6223810315132141, D(G(z)) 0.5566516518592834\n",
      "G training 4, loss 0.5636899471282959, D(G(z)) 0.5752394795417786\n",
      "\n",
      "\n",
      "[0/3][73/100]\tLoss_D: 2.8860\tLoss_G: 0.5637\tD(x): 0.3024\tD(G(z)): 0.7529 / 0.5752\n",
      "g_grad:  tensor(0.0005) d_grad tensor(-0.0008)\n",
      "G training 1, loss 0.8716598749160767, D(G(z)) 0.48962563276290894\n",
      "G training 2, loss 1.276628851890564, D(G(z)) 0.3311474323272705\n",
      "G training 3, loss 0.6119835376739502, D(G(z)) 0.6127283573150635\n",
      "G training 4, loss 0.9254607558250427, D(G(z)) 0.4307783246040344\n",
      "\n",
      "\n",
      "[0/3][74/100]\tLoss_D: 2.7013\tLoss_G: 0.9255\tD(x): 0.3421\tD(G(z)): 0.7357 / 0.4308\n",
      "g_grad:  tensor(0.0032) d_grad tensor(0.0705)\n",
      "G training 1, loss 0.8014975190162659, D(G(z)) 0.4887799918651581\n",
      "G training 2, loss 0.6565913558006287, D(G(z)) 0.528764009475708\n",
      "G training 3, loss 0.6257764101028442, D(G(z)) 0.5484081506729126\n",
      "G training 4, loss 0.4441456198692322, D(G(z)) 0.6460798382759094\n",
      "\n",
      "\n",
      "[0/3][75/100]\tLoss_D: 3.4752\tLoss_G: 0.4441\tD(x): 0.1456\tD(G(z)): 0.5456 / 0.6461\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0959)\n",
      "G training 1, loss 0.7297216653823853, D(G(z)) 0.505101203918457\n",
      "G training 2, loss 0.7040639519691467, D(G(z)) 0.532734215259552\n",
      "G training 3, loss 0.46037396788597107, D(G(z)) 0.6410493850708008\n",
      "G training 4, loss 0.39122653007507324, D(G(z)) 0.6797332763671875\n",
      "\n",
      "\n",
      "[0/3][76/100]\tLoss_D: 2.7220\tLoss_G: 0.3912\tD(x): 0.2488\tD(G(z)): 0.6329 / 0.6797\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(0.0857)\n",
      "G training 1, loss 0.6053261756896973, D(G(z)) 0.5792685151100159\n",
      "G training 2, loss 0.4802585542201996, D(G(z)) 0.6324949264526367\n",
      "G training 3, loss 0.7895508408546448, D(G(z)) 0.4764860272407532\n",
      "G training 4, loss 0.4493235945701599, D(G(z)) 0.6514163017272949\n",
      "\n",
      "\n",
      "[0/3][77/100]\tLoss_D: 4.1125\tLoss_G: 0.4493\tD(x): 0.1081\tD(G(z)): 0.6581 / 0.6514\n",
      "g_grad:  tensor(5.2557e-05) d_grad tensor(0.0793)\n",
      "G training 1, loss 0.8701115846633911, D(G(z)) 0.42410025000572205\n",
      "G training 2, loss 0.6144436001777649, D(G(z)) 0.5435085892677307\n",
      "G training 3, loss 0.7756462693214417, D(G(z)) 0.4831317067146301\n",
      "G training 4, loss 0.6977007985115051, D(G(z)) 0.5186816453933716\n",
      "\n",
      "\n",
      "[0/3][78/100]\tLoss_D: 2.2662\tLoss_G: 0.6977\tD(x): 0.4686\tD(G(z)): 0.7250 / 0.5187\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(0.0654)\n",
      "G training 1, loss 0.7541796565055847, D(G(z)) 0.5148155689239502\n",
      "G training 2, loss 0.575923502445221, D(G(z)) 0.569521963596344\n",
      "G training 3, loss 0.6744820475578308, D(G(z)) 0.5244227051734924\n",
      "G training 4, loss 0.6150380373001099, D(G(z)) 0.5807636976242065\n",
      "\n",
      "\n",
      "[0/3][79/100]\tLoss_D: 1.8277\tLoss_G: 0.6150\tD(x): 0.4243\tD(G(z)): 0.5584 / 0.5808\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0339)\n",
      "G training 1, loss 0.989966630935669, D(G(z)) 0.4839368164539337\n",
      "G training 2, loss 0.6406245231628418, D(G(z)) 0.5451556444168091\n",
      "G training 3, loss 1.0125150680541992, D(G(z)) 0.393128365278244\n",
      "G training 4, loss 0.6385042071342468, D(G(z)) 0.5813519954681396\n",
      "\n",
      "\n",
      "[0/3][80/100]\tLoss_D: 2.0708\tLoss_G: 0.6385\tD(x): 0.3731\tD(G(z)): 0.6339 / 0.5814\n",
      "g_grad:  tensor(0.0012) d_grad tensor(0.0175)\n",
      "G training 1, loss 0.6044795513153076, D(G(z)) 0.5497115850448608\n",
      "G training 2, loss 0.6405025720596313, D(G(z)) 0.5704394578933716\n",
      "G training 3, loss 0.5327088832855225, D(G(z)) 0.5964036583900452\n",
      "G training 4, loss 0.7215660214424133, D(G(z)) 0.5314228534698486\n",
      "\n",
      "\n",
      "[0/3][81/100]\tLoss_D: 1.9891\tLoss_G: 0.7216\tD(x): 0.4087\tD(G(z)): 0.6314 / 0.5314\n",
      "g_grad:  tensor(3.8713e-05) d_grad tensor(0.0420)\n",
      "G training 1, loss 0.8278199434280396, D(G(z)) 0.4629189670085907\n",
      "G training 2, loss 0.5712522864341736, D(G(z)) 0.5735719799995422\n",
      "G training 3, loss 0.4438421428203583, D(G(z)) 0.6452622413635254\n",
      "G training 4, loss 0.4037529528141022, D(G(z)) 0.6697195172309875\n",
      "\n",
      "\n",
      "[0/3][82/100]\tLoss_D: 1.9748\tLoss_G: 0.4038\tD(x): 0.5672\tD(G(z)): 0.7419 / 0.6697\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0126)\n",
      "G training 1, loss 0.7901443243026733, D(G(z)) 0.4585783779621124\n",
      "G training 2, loss 0.922251284122467, D(G(z)) 0.430595725774765\n",
      "G training 3, loss 0.5958245992660522, D(G(z)) 0.5595127940177917\n",
      "G training 4, loss 0.7212933897972107, D(G(z)) 0.5043607354164124\n",
      "\n",
      "\n",
      "[0/3][83/100]\tLoss_D: 2.0278\tLoss_G: 0.7213\tD(x): 0.4145\tD(G(z)): 0.6194 / 0.5044\n",
      "g_grad:  tensor(0.0009) d_grad tensor(0.0040)\n",
      "G training 1, loss 1.1250256299972534, D(G(z)) 0.346213698387146\n",
      "G training 2, loss 0.7857919931411743, D(G(z)) 0.4752461314201355\n",
      "G training 3, loss 0.4907020032405853, D(G(z)) 0.6223960518836975\n",
      "G training 4, loss 0.48378604650497437, D(G(z)) 0.6242746710777283\n",
      "\n",
      "\n",
      "[0/3][84/100]\tLoss_D: 1.7575\tLoss_G: 0.4838\tD(x): 0.5130\tD(G(z)): 0.6362 / 0.6243\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0343)\n",
      "G training 1, loss 1.1298408508300781, D(G(z)) 0.3434128165245056\n",
      "G training 2, loss 0.859548807144165, D(G(z)) 0.4841807782649994\n",
      "G training 3, loss 0.7971966862678528, D(G(z)) 0.4791153073310852\n",
      "G training 4, loss 0.6212344765663147, D(G(z)) 0.5475622415542603\n",
      "\n",
      "\n",
      "[0/3][85/100]\tLoss_D: 2.2635\tLoss_G: 0.6212\tD(x): 0.3861\tD(G(z)): 0.6684 / 0.5476\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(0.0472)\n",
      "G training 1, loss 0.5579644441604614, D(G(z)) 0.576779305934906\n",
      "G training 2, loss 0.4131728708744049, D(G(z)) 0.6638258695602417\n",
      "G training 3, loss 0.5678633451461792, D(G(z)) 0.5737091302871704\n",
      "G training 4, loss 0.3325878381729126, D(G(z)) 0.7186948657035828\n",
      "\n",
      "\n",
      "[0/3][86/100]\tLoss_D: 2.1959\tLoss_G: 0.3326\tD(x): 0.3429\tD(G(z)): 0.6129 / 0.7187\n",
      "g_grad:  tensor(1.4109e-05) d_grad tensor(-0.0092)\n",
      "G training 1, loss 0.8780689239501953, D(G(z)) 0.4283533990383148\n",
      "G training 2, loss 0.5138898491859436, D(G(z)) 0.6095392107963562\n",
      "G training 3, loss 0.7115409970283508, D(G(z)) 0.5561310052871704\n",
      "G training 4, loss 0.48509013652801514, D(G(z)) 0.6188488602638245\n",
      "\n",
      "\n",
      "[0/3][87/100]\tLoss_D: 2.3166\tLoss_G: 0.4851\tD(x): 0.4043\tD(G(z)): 0.7344 / 0.6188\n",
      "g_grad:  tensor(-4.5329e-05) d_grad tensor(-0.0570)\n",
      "G training 1, loss 1.4168974161148071, D(G(z)) 0.2633810043334961\n",
      "G training 2, loss 0.9034873247146606, D(G(z)) 0.4231681227684021\n",
      "G training 3, loss 0.680039644241333, D(G(z)) 0.5329594612121582\n",
      "G training 4, loss 0.4943574368953705, D(G(z)) 0.6188515424728394\n",
      "\n",
      "\n",
      "[0/3][88/100]\tLoss_D: 2.7637\tLoss_G: 0.4944\tD(x): 0.3589\tD(G(z)): 0.7937 / 0.6189\n",
      "g_grad:  tensor(-5.4876e-05) d_grad tensor(-0.0411)\n",
      "G training 1, loss 0.8331301212310791, D(G(z)) 0.45753854513168335\n",
      "G training 2, loss 1.5674346685409546, D(G(z)) 0.24164161086082458\n",
      "G training 3, loss 1.200858235359192, D(G(z)) 0.3066978454589844\n",
      "G training 4, loss 0.5526881217956543, D(G(z)) 0.5785014629364014\n",
      "\n",
      "\n",
      "[0/3][89/100]\tLoss_D: 2.9134\tLoss_G: 0.5527\tD(x): 0.2016\tD(G(z)): 0.6909 / 0.5785\n",
      "g_grad:  tensor(-0.0032) d_grad tensor(0.0099)\n",
      "G training 1, loss 1.140755295753479, D(G(z)) 0.34653353691101074\n",
      "G training 2, loss 0.49908381700515747, D(G(z)) 0.612724781036377\n",
      "G training 3, loss 0.459220290184021, D(G(z)) 0.634674072265625\n",
      "G training 4, loss 0.33922022581100464, D(G(z)) 0.7141637206077576\n",
      "\n",
      "\n",
      "[0/3][90/100]\tLoss_D: 2.2378\tLoss_G: 0.3392\tD(x): 0.2459\tD(G(z)): 0.4084 / 0.7142\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0370)\n",
      "G training 1, loss 0.7838378548622131, D(G(z)) 0.4666176438331604\n",
      "G training 2, loss 0.5694733262062073, D(G(z)) 0.582344114780426\n",
      "G training 3, loss 0.5446282625198364, D(G(z)) 0.5999205112457275\n",
      "G training 4, loss 0.3476911187171936, D(G(z)) 0.714072585105896\n",
      "\n",
      "\n",
      "[0/3][91/100]\tLoss_D: 2.7569\tLoss_G: 0.3477\tD(x): 0.2739\tD(G(z)): 0.7224 / 0.7141\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0377)\n",
      "G training 1, loss 1.2516967058181763, D(G(z)) 0.29896625876426697\n",
      "G training 2, loss 0.6567991375923157, D(G(z)) 0.5463494062423706\n",
      "G training 3, loss 0.5035433769226074, D(G(z)) 0.6093264818191528\n",
      "G training 4, loss 0.633590042591095, D(G(z)) 0.552705705165863\n",
      "\n",
      "\n",
      "[0/3][92/100]\tLoss_D: 2.5450\tLoss_G: 0.6336\tD(x): 0.4039\tD(G(z)): 0.7902 / 0.5527\n",
      "g_grad:  tensor(-3.1457e-05) d_grad tensor(0.0038)\n",
      "G training 1, loss 1.0331032276153564, D(G(z)) 0.3749229311943054\n",
      "G training 2, loss 0.9809249639511108, D(G(z)) 0.37585535645484924\n",
      "G training 3, loss 0.7723161578178406, D(G(z)) 0.4827466905117035\n",
      "G training 4, loss 0.5190010070800781, D(G(z)) 0.5994125008583069\n",
      "\n",
      "\n",
      "[0/3][93/100]\tLoss_D: 2.4724\tLoss_G: 0.5190\tD(x): 0.2978\tD(G(z)): 0.6489 / 0.5994\n",
      "g_grad:  tensor(-0.0008) d_grad tensor(0.0071)\n",
      "G training 1, loss 0.9015780687332153, D(G(z)) 0.43121132254600525\n",
      "G training 2, loss 0.6031044721603394, D(G(z)) 0.560359001159668\n",
      "G training 3, loss 0.7034742832183838, D(G(z)) 0.530728280544281\n",
      "G training 4, loss 0.6125651597976685, D(G(z)) 0.5789275765419006\n",
      "\n",
      "\n",
      "[0/3][94/100]\tLoss_D: 1.8852\tLoss_G: 0.6126\tD(x): 0.4464\tD(G(z)): 0.6283 / 0.5789\n",
      "g_grad:  tensor(0.0007) d_grad tensor(0.0120)\n",
      "G training 1, loss 0.7587102651596069, D(G(z)) 0.4835153818130493\n",
      "G training 2, loss 0.7171018123626709, D(G(z)) 0.5011399984359741\n",
      "G training 3, loss 0.783795952796936, D(G(z)) 0.4626147150993347\n",
      "G training 4, loss 0.8152883648872375, D(G(z)) 0.45383310317993164\n",
      "\n",
      "\n",
      "[0/3][95/100]\tLoss_D: 2.4101\tLoss_G: 0.8153\tD(x): 0.2669\tD(G(z)): 0.5896 / 0.4538\n",
      "g_grad:  tensor(2.6828e-05) d_grad tensor(-0.0392)\n",
      "G training 1, loss 0.5124627351760864, D(G(z)) 0.6100366115570068\n",
      "G training 2, loss 0.7727559208869934, D(G(z)) 0.46961745619773865\n",
      "G training 3, loss 0.7384690046310425, D(G(z)) 0.4833540618419647\n",
      "G training 4, loss 0.4197945296764374, D(G(z)) 0.6582596302032471\n",
      "\n",
      "\n",
      "[0/3][96/100]\tLoss_D: 3.3992\tLoss_G: 0.4198\tD(x): 0.0796\tD(G(z)): 0.5501 / 0.6583\n",
      "g_grad:  tensor(8.8005e-05) d_grad tensor(-0.0164)\n",
      "G training 1, loss 0.41067367792129517, D(G(z)) 0.6679076552391052\n",
      "G training 2, loss 0.6075829863548279, D(G(z)) 0.5570164918899536\n",
      "G training 3, loss 0.7128756642341614, D(G(z)) 0.5257449746131897\n",
      "G training 4, loss 0.3347909152507782, D(G(z)) 0.7211357951164246\n",
      "\n",
      "\n",
      "[0/3][97/100]\tLoss_D: 2.5756\tLoss_G: 0.3348\tD(x): 0.2315\tD(G(z)): 0.5728 / 0.7211\n",
      "g_grad:  tensor(0.0004) d_grad tensor(-0.0045)\n",
      "G training 1, loss 0.6602638363838196, D(G(z)) 0.540274977684021\n",
      "G training 2, loss 1.0261708498001099, D(G(z)) 0.39020928740501404\n",
      "G training 3, loss 0.5599855780601501, D(G(z)) 0.5775562524795532\n",
      "G training 4, loss 0.49897047877311707, D(G(z)) 0.617387592792511\n",
      "\n",
      "\n",
      "[0/3][98/100]\tLoss_D: 2.4137\tLoss_G: 0.4990\tD(x): 0.3409\tD(G(z)): 0.7143 / 0.6174\n",
      "g_grad:  tensor(-8.1298e-05) d_grad tensor(-0.0100)\n",
      "G training 1, loss 0.4975277781486511, D(G(z)) 0.6128391027450562\n",
      "G training 2, loss 0.6810567378997803, D(G(z)) 0.5514301061630249\n",
      "G training 3, loss 0.5397035479545593, D(G(z)) 0.5968531966209412\n",
      "G training 4, loss 0.7328657507896423, D(G(z)) 0.5411218404769897\n",
      "\n",
      "\n",
      "[0/3][99/100]\tLoss_D: 1.5789\tLoss_G: 0.7329\tD(x): 0.5541\tD(G(z)): 0.5928 / 0.5411\n",
      "g_grad:  tensor(0.0001) d_grad tensor(-0.0030)\n",
      "G training 1, loss 0.6035736799240112, D(G(z)) 0.5525749325752258\n",
      "G training 2, loss 0.5402379035949707, D(G(z)) 0.5901526212692261\n",
      "G training 3, loss 0.3623661994934082, D(G(z)) 0.703768253326416\n",
      "G training 4, loss 0.4459918141365051, D(G(z)) 0.6603357195854187\n",
      "\n",
      "\n",
      "[1/3][0/100]\tLoss_D: 1.7541\tLoss_G: 0.4460\tD(x): 0.5749\tD(G(z)): 0.6280 / 0.6603\n",
      "g_grad:  tensor(0.0001) d_grad tensor(-0.0069)\n",
      "G training 1, loss 0.7771915197372437, D(G(z)) 0.5088893175125122\n",
      "G training 2, loss 0.38160452246665955, D(G(z)) 0.6876753568649292\n",
      "G training 3, loss 0.5775316953659058, D(G(z)) 0.577000617980957\n",
      "G training 4, loss 0.37250906229019165, D(G(z)) 0.6896536946296692\n",
      "\n",
      "\n",
      "[1/3][1/100]\tLoss_D: 2.0686\tLoss_G: 0.3725\tD(x): 0.4265\tD(G(z)): 0.6825 / 0.6897\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0050)\n",
      "G training 1, loss 0.4988647997379303, D(G(z)) 0.6276982426643372\n",
      "G training 2, loss 0.3790580630302429, D(G(z)) 0.6886791586875916\n",
      "G training 3, loss 0.385907918214798, D(G(z)) 0.6894057989120483\n",
      "G training 4, loss 0.5091451406478882, D(G(z)) 0.6171677708625793\n",
      "\n",
      "\n",
      "[1/3][2/100]\tLoss_D: 1.9814\tLoss_G: 0.5091\tD(x): 0.4390\tD(G(z)): 0.6557 / 0.6172\n",
      "g_grad:  tensor(3.3934e-05) d_grad tensor(-0.0051)\n",
      "G training 1, loss 0.5264365673065186, D(G(z)) 0.5983532667160034\n",
      "G training 2, loss 0.6718122363090515, D(G(z)) 0.518168568611145\n",
      "G training 3, loss 0.5816752314567566, D(G(z)) 0.5629632472991943\n",
      "G training 4, loss 0.6647012829780579, D(G(z)) 0.5248743295669556\n",
      "\n",
      "\n",
      "[1/3][3/100]\tLoss_D: 2.0057\tLoss_G: 0.6647\tD(x): 0.4667\tD(G(z)): 0.6317 / 0.5249\n",
      "g_grad:  tensor(0.0005) d_grad tensor(-0.0085)\n",
      "G training 1, loss 0.5182780623435974, D(G(z)) 0.5971258878707886\n",
      "G training 2, loss 0.6249045729637146, D(G(z)) 0.5445520281791687\n",
      "G training 3, loss 0.650932788848877, D(G(z)) 0.533963680267334\n",
      "G training 4, loss 0.4376136362552643, D(G(z)) 0.6467896699905396\n",
      "\n",
      "\n",
      "[1/3][4/100]\tLoss_D: 1.9311\tLoss_G: 0.4376\tD(x): 0.4395\tD(G(z)): 0.6263 / 0.6468\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0007)\n",
      "G training 1, loss 0.5568240284919739, D(G(z)) 0.5736472010612488\n",
      "G training 2, loss 0.5015299320220947, D(G(z)) 0.606376051902771\n",
      "G training 3, loss 0.4151119589805603, D(G(z)) 0.6609634757041931\n",
      "G training 4, loss 0.3725096583366394, D(G(z)) 0.6911180019378662\n",
      "\n",
      "\n",
      "[1/3][5/100]\tLoss_D: 1.7871\tLoss_G: 0.3725\tD(x): 0.3685\tD(G(z)): 0.5077 / 0.6911\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(-0.0343)\n",
      "G training 1, loss 0.543685793876648, D(G(z)) 0.5848339796066284\n",
      "G training 2, loss 0.5224704742431641, D(G(z)) 0.6003226041793823\n",
      "G training 3, loss 0.4361046552658081, D(G(z)) 0.6503822803497314\n",
      "G training 4, loss 0.6372899413108826, D(G(z)) 0.5326673984527588\n",
      "\n",
      "\n",
      "[1/3][6/100]\tLoss_D: 1.8097\tLoss_G: 0.6373\tD(x): 0.5155\tD(G(z)): 0.6585 / 0.5327\n",
      "g_grad:  tensor(0.0002) d_grad tensor(-0.0298)\n",
      "G training 1, loss 0.6706276535987854, D(G(z)) 0.5158873796463013\n",
      "G training 2, loss 0.5078274011611938, D(G(z)) 0.6040186285972595\n",
      "G training 3, loss 0.4451993405818939, D(G(z)) 0.6422715783119202\n",
      "G training 4, loss 0.4859308898448944, D(G(z)) 0.618403434753418\n",
      "\n",
      "\n",
      "[1/3][7/100]\tLoss_D: 1.8675\tLoss_G: 0.4859\tD(x): 0.4227\tD(G(z)): 0.6086 / 0.6184\n",
      "g_grad:  tensor(0.0011) d_grad tensor(-0.0242)\n",
      "G training 1, loss 0.5803232789039612, D(G(z)) 0.5607699751853943\n",
      "G training 2, loss 0.7967812418937683, D(G(z)) 0.45481669902801514\n",
      "G training 3, loss 0.5009152889251709, D(G(z)) 0.6080581545829773\n",
      "G training 4, loss 0.5623840689659119, D(G(z)) 0.5728656053543091\n",
      "\n",
      "\n",
      "[1/3][8/100]\tLoss_D: 1.9747\tLoss_G: 0.5624\tD(x): 0.3525\tD(G(z)): 0.5906 / 0.5729\n",
      "g_grad:  tensor(0.0011) d_grad tensor(-0.0223)\n",
      "G training 1, loss 0.6250807642936707, D(G(z)) 0.5440258383750916\n",
      "G training 2, loss 0.520892322063446, D(G(z)) 0.607046365737915\n",
      "G training 3, loss 0.5624773502349854, D(G(z)) 0.5802072882652283\n",
      "G training 4, loss 0.3638434112071991, D(G(z)) 0.6971246004104614\n",
      "\n",
      "\n",
      "[1/3][9/100]\tLoss_D: 3.1498\tLoss_G: 0.3638\tD(x): 0.1479\tD(G(z)): 0.5684 / 0.6971\n",
      "g_grad:  tensor(-4.5268e-05) d_grad tensor(-0.0178)\n",
      "G training 1, loss 0.5131232142448425, D(G(z)) 0.6041730046272278\n",
      "G training 2, loss 0.5830384492874146, D(G(z)) 0.5648828148841858\n",
      "G training 3, loss 0.46482640504837036, D(G(z)) 0.6421080827713013\n",
      "G training 4, loss 0.5942160487174988, D(G(z)) 0.5694876909255981\n",
      "\n",
      "\n",
      "[1/3][10/100]\tLoss_D: 2.3003\tLoss_G: 0.5942\tD(x): 0.3780\tD(G(z)): 0.7159 / 0.5695\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(-0.0012)\n",
      "G training 1, loss 0.601852536201477, D(G(z)) 0.5613865852355957\n",
      "G training 2, loss 0.3782774806022644, D(G(z)) 0.6854199171066284\n",
      "G training 3, loss 0.6862910389900208, D(G(z)) 0.5166335105895996\n",
      "G training 4, loss 0.4250427186489105, D(G(z)) 0.6568685173988342\n",
      "\n",
      "\n",
      "[1/3][11/100]\tLoss_D: 2.1758\tLoss_G: 0.4250\tD(x): 0.4584\tD(G(z)): 0.7260 / 0.6569\n",
      "g_grad:  tensor(5.7267e-05) d_grad tensor(0.0230)\n",
      "G training 1, loss 0.6139812469482422, D(G(z)) 0.5445019006729126\n",
      "G training 2, loss 0.41096219420433044, D(G(z)) 0.6651198863983154\n",
      "G training 3, loss 0.35852912068367004, D(G(z)) 0.6991764307022095\n",
      "G training 4, loss 0.5424882769584656, D(G(z)) 0.6182984113693237\n",
      "\n",
      "\n",
      "[1/3][12/100]\tLoss_D: 3.2030\tLoss_G: 0.5425\tD(x): 0.1399\tD(G(z)): 0.5844 / 0.6183\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0118)\n",
      "G training 1, loss 0.932776153087616, D(G(z)) 0.41958126425743103\n",
      "G training 2, loss 0.3998388350009918, D(G(z)) 0.6736756563186646\n",
      "G training 3, loss 0.4391998052597046, D(G(z)) 0.6509571075439453\n",
      "G training 4, loss 0.3146260380744934, D(G(z)) 0.7373796701431274\n",
      "\n",
      "\n",
      "[1/3][13/100]\tLoss_D: 2.0101\tLoss_G: 0.3146\tD(x): 0.4876\tD(G(z)): 0.6841 / 0.7374\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0107)\n",
      "G training 1, loss 0.6697386503219604, D(G(z)) 0.5247231721878052\n",
      "G training 2, loss 0.7970417737960815, D(G(z)) 0.46438759565353394\n",
      "G training 3, loss 0.5772573947906494, D(G(z)) 0.5791452527046204\n",
      "G training 4, loss 0.4183850884437561, D(G(z)) 0.6638534069061279\n",
      "\n",
      "\n",
      "[1/3][14/100]\tLoss_D: 1.5863\tLoss_G: 0.4184\tD(x): 0.5573\tD(G(z)): 0.6229 / 0.6639\n",
      "g_grad:  tensor(8.8803e-05) d_grad tensor(0.0159)\n",
      "G training 1, loss 0.6123386025428772, D(G(z)) 0.55672687292099\n",
      "G training 2, loss 0.5702897906303406, D(G(z)) 0.5863139629364014\n",
      "G training 3, loss 0.6574758887290955, D(G(z)) 0.5245733857154846\n",
      "G training 4, loss 0.5328739881515503, D(G(z)) 0.6089413166046143\n",
      "\n",
      "\n",
      "[1/3][15/100]\tLoss_D: 2.8674\tLoss_G: 0.5329\tD(x): 0.1748\tD(G(z)): 0.5885 / 0.6089\n",
      "g_grad:  tensor(-0.0006) d_grad tensor(0.0167)\n",
      "G training 1, loss 0.5318871140480042, D(G(z)) 0.5981283783912659\n",
      "G training 2, loss 0.4302934408187866, D(G(z)) 0.6578620672225952\n",
      "G training 3, loss 0.5646052956581116, D(G(z)) 0.591711163520813\n",
      "G training 4, loss 0.5255740284919739, D(G(z)) 0.6050781011581421\n",
      "\n",
      "\n",
      "[1/3][16/100]\tLoss_D: 2.3303\tLoss_G: 0.5256\tD(x): 0.2932\tD(G(z)): 0.6099 / 0.6051\n",
      "g_grad:  tensor(2.9616e-05) d_grad tensor(0.0180)\n",
      "G training 1, loss 0.5828176736831665, D(G(z)) 0.5624009966850281\n",
      "G training 2, loss 0.4127802848815918, D(G(z)) 0.6641100645065308\n",
      "G training 3, loss 0.8928700089454651, D(G(z)) 0.5342610478401184\n",
      "G training 4, loss 0.4062066078186035, D(G(z)) 0.6676130294799805\n",
      "\n",
      "\n",
      "[1/3][17/100]\tLoss_D: 2.3556\tLoss_G: 0.4062\tD(x): 0.4877\tD(G(z)): 0.7318 / 0.6676\n",
      "g_grad:  tensor(-5.8958e-05) d_grad tensor(0.0179)\n",
      "G training 1, loss 0.8300210237503052, D(G(z)) 0.4878363609313965\n",
      "G training 2, loss 0.5103563666343689, D(G(z)) 0.6097348928451538\n",
      "G training 3, loss 0.5964458584785461, D(G(z)) 0.5626488924026489\n",
      "G training 4, loss 0.4338599145412445, D(G(z)) 0.6525424122810364\n",
      "\n",
      "\n",
      "[1/3][18/100]\tLoss_D: 1.6719\tLoss_G: 0.4339\tD(x): 0.5829\tD(G(z)): 0.6746 / 0.6525\n",
      "g_grad:  tensor(-6.0374e-06) d_grad tensor(0.0201)\n",
      "G training 1, loss 0.5917794108390808, D(G(z)) 0.5658434629440308\n",
      "G training 2, loss 0.46453428268432617, D(G(z)) 0.6341375112533569\n",
      "G training 3, loss 0.791485607624054, D(G(z)) 0.4948301911354065\n",
      "G training 4, loss 0.43172430992126465, D(G(z)) 0.6561517119407654\n",
      "\n",
      "\n",
      "[1/3][19/100]\tLoss_D: 1.7968\tLoss_G: 0.4317\tD(x): 0.4866\tD(G(z)): 0.6391 / 0.6562\n",
      "g_grad:  tensor(4.1052e-05) d_grad tensor(0.0141)\n",
      "G training 1, loss 0.7798162698745728, D(G(z)) 0.4780237674713135\n",
      "G training 2, loss 0.533423125743866, D(G(z)) 0.601957380771637\n",
      "G training 3, loss 0.6351615190505981, D(G(z)) 0.5347409844398499\n",
      "G training 4, loss 0.49838149547576904, D(G(z)) 0.6261590123176575\n",
      "\n",
      "\n",
      "[1/3][20/100]\tLoss_D: 2.5365\tLoss_G: 0.4984\tD(x): 0.2726\tD(G(z)): 0.6767 / 0.6262\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0189)\n",
      "G training 1, loss 0.7027925252914429, D(G(z)) 0.5126670598983765\n",
      "G training 2, loss 0.529765248298645, D(G(z)) 0.5900031328201294\n",
      "G training 3, loss 0.5721367597579956, D(G(z)) 0.5661357045173645\n",
      "G training 4, loss 0.5561851263046265, D(G(z)) 0.58510822057724\n",
      "\n",
      "\n",
      "[1/3][21/100]\tLoss_D: 2.1473\tLoss_G: 0.5562\tD(x): 0.4561\tD(G(z)): 0.6884 / 0.5851\n",
      "g_grad:  tensor(8.9613e-05) d_grad tensor(0.0136)\n",
      "G training 1, loss 0.8609687089920044, D(G(z)) 0.4241882264614105\n",
      "G training 2, loss 1.3622753620147705, D(G(z)) 0.2618143856525421\n",
      "G training 3, loss 0.9309467077255249, D(G(z)) 0.39777129888534546\n",
      "G training 4, loss 0.8432341814041138, D(G(z)) 0.4313213527202606\n",
      "\n",
      "\n",
      "[1/3][22/100]\tLoss_D: 2.9251\tLoss_G: 0.8432\tD(x): 0.1894\tD(G(z)): 0.5991 / 0.4313\n",
      "g_grad:  tensor(7.8046e-05) d_grad tensor(-0.0008)\n",
      "G training 1, loss 0.9150136113166809, D(G(z)) 0.4048011302947998\n",
      "G training 2, loss 0.6449896097183228, D(G(z)) 0.5258319973945618\n",
      "G training 3, loss 0.6676162481307983, D(G(z)) 0.5143704414367676\n",
      "G training 4, loss 0.7257146835327148, D(G(z)) 0.4857073724269867\n",
      "\n",
      "\n",
      "[1/3][23/100]\tLoss_D: 1.3347\tLoss_G: 0.7257\tD(x): 0.3984\tD(G(z)): 0.3213 / 0.4857\n",
      "g_grad:  tensor(-6.2129e-05) d_grad tensor(-0.0071)\n",
      "G training 1, loss 0.6400243639945984, D(G(z)) 0.5305099487304688\n",
      "G training 2, loss 0.6032377481460571, D(G(z)) 0.552056074142456\n",
      "G training 3, loss 0.6048775911331177, D(G(z)) 0.5515386462211609\n",
      "G training 4, loss 0.6264848709106445, D(G(z)) 0.5452282428741455\n",
      "\n",
      "\n",
      "[1/3][24/100]\tLoss_D: 1.4491\tLoss_G: 0.6265\tD(x): 0.5419\tD(G(z)): 0.5615 / 0.5452\n",
      "g_grad:  tensor(-1.9463e-06) d_grad tensor(-0.0160)\n",
      "G training 1, loss 0.7339267134666443, D(G(z)) 0.4866863489151001\n",
      "G training 2, loss 0.4554356038570404, D(G(z)) 0.6355419158935547\n",
      "G training 3, loss 0.5051419734954834, D(G(z)) 0.6059076189994812\n",
      "G training 4, loss 0.5630465149879456, D(G(z)) 0.5748027563095093\n",
      "\n",
      "\n",
      "[1/3][25/100]\tLoss_D: 1.7757\tLoss_G: 0.5630\tD(x): 0.4074\tD(G(z)): 0.5609 / 0.5748\n",
      "g_grad:  tensor(0.0001) d_grad tensor(-0.0122)\n",
      "G training 1, loss 0.6265995502471924, D(G(z)) 0.5455560088157654\n",
      "G training 2, loss 0.5204682350158691, D(G(z)) 0.6009587049484253\n",
      "G training 3, loss 0.4731924533843994, D(G(z)) 0.6254568696022034\n",
      "G training 4, loss 0.5503493547439575, D(G(z)) 0.5879203081130981\n",
      "\n",
      "\n",
      "[1/3][26/100]\tLoss_D: 1.8197\tLoss_G: 0.5503\tD(x): 0.4695\tD(G(z)): 0.6367 / 0.5879\n",
      "g_grad:  tensor(-0.0004) d_grad tensor(-0.0148)\n",
      "G training 1, loss 0.4570237994194031, D(G(z)) 0.6339685916900635\n",
      "G training 2, loss 0.5749610662460327, D(G(z)) 0.5668589472770691\n",
      "G training 3, loss 0.5254383087158203, D(G(z)) 0.5968077182769775\n",
      "G training 4, loss 0.39295580983161926, D(G(z)) 0.6797207593917847\n",
      "\n",
      "\n",
      "[1/3][27/100]\tLoss_D: 2.3918\tLoss_G: 0.3930\tD(x): 0.2590\tD(G(z)): 0.5667 / 0.6797\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(0.0065)\n",
      "G training 1, loss 0.5722869634628296, D(G(z)) 0.5671079158782959\n",
      "G training 2, loss 0.5505472421646118, D(G(z)) 0.5868607759475708\n",
      "G training 3, loss 0.5059200525283813, D(G(z)) 0.6049404740333557\n",
      "G training 4, loss 0.4585167169570923, D(G(z)) 0.6363188028335571\n",
      "\n",
      "\n",
      "[1/3][28/100]\tLoss_D: 2.0765\tLoss_G: 0.4585\tD(x): 0.4012\tD(G(z)): 0.6529 / 0.6363\n",
      "g_grad:  tensor(5.5145e-05) d_grad tensor(-0.0083)\n",
      "G training 1, loss 0.8680311441421509, D(G(z)) 0.4265446066856384\n",
      "G training 2, loss 0.4408568739891052, D(G(z)) 0.6446521282196045\n",
      "G training 3, loss 0.530072033405304, D(G(z)) 0.5960226058959961\n",
      "G training 4, loss 0.416423499584198, D(G(z)) 0.6633211970329285\n",
      "\n",
      "\n",
      "[1/3][29/100]\tLoss_D: 1.5874\tLoss_G: 0.4164\tD(x): 0.5666\tD(G(z)): 0.6298 / 0.6633\n",
      "g_grad:  tensor(-2.9129e-05) d_grad tensor(-0.0162)\n",
      "G training 1, loss 0.5498030185699463, D(G(z)) 0.5816682577133179\n",
      "G training 2, loss 0.4971848428249359, D(G(z)) 0.6107739210128784\n",
      "G training 3, loss 0.5575388073921204, D(G(z)) 0.5765777826309204\n",
      "G training 4, loss 0.515005886554718, D(G(z)) 0.5993183255195618\n",
      "\n",
      "\n",
      "[1/3][30/100]\tLoss_D: 1.7135\tLoss_G: 0.5150\tD(x): 0.4572\tD(G(z)): 0.5867 / 0.5993\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(-0.0180)\n",
      "G training 1, loss 0.5991241335868835, D(G(z)) 0.5634819269180298\n",
      "G training 2, loss 0.6794034242630005, D(G(z)) 0.5492523312568665\n",
      "G training 3, loss 0.4699976444244385, D(G(z)) 0.6287983655929565\n",
      "G training 4, loss 0.6572472453117371, D(G(z)) 0.5438486933708191\n",
      "\n",
      "\n",
      "[1/3][31/100]\tLoss_D: 2.6438\tLoss_G: 0.6572\tD(x): 0.2579\tD(G(z)): 0.6579 / 0.5438\n",
      "g_grad:  tensor(0.0002) d_grad tensor(-0.0222)\n",
      "G training 1, loss 0.8275968432426453, D(G(z)) 0.4548017680644989\n",
      "G training 2, loss 0.6926561594009399, D(G(z)) 0.5355275869369507\n",
      "G training 3, loss 0.5420029759407043, D(G(z)) 0.6120094656944275\n",
      "G training 4, loss 0.5859261751174927, D(G(z)) 0.5715094208717346\n",
      "\n",
      "\n",
      "[1/3][32/100]\tLoss_D: 2.2436\tLoss_G: 0.5859\tD(x): 0.4125\tD(G(z)): 0.7204 / 0.5715\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(-0.0181)\n",
      "G training 1, loss 0.5884790420532227, D(G(z)) 0.5558770895004272\n",
      "G training 2, loss 0.5089151263237, D(G(z)) 0.6076986789703369\n",
      "G training 3, loss 0.7830603718757629, D(G(z)) 0.4762188792228699\n",
      "G training 4, loss 0.5575479865074158, D(G(z)) 0.5799518823623657\n",
      "\n",
      "\n",
      "[1/3][33/100]\tLoss_D: 2.0803\tLoss_G: 0.5575\tD(x): 0.4078\tD(G(z)): 0.6759 / 0.5800\n",
      "g_grad:  tensor(3.1583e-05) d_grad tensor(-0.0066)\n",
      "G training 1, loss 0.6328821182250977, D(G(z)) 0.5332803726196289\n",
      "G training 2, loss 0.6139427423477173, D(G(z)) 0.5582336187362671\n",
      "G training 3, loss 0.6056764721870422, D(G(z)) 0.5536015033721924\n",
      "G training 4, loss 0.9399901628494263, D(G(z)) 0.43475955724716187\n",
      "\n",
      "\n",
      "[1/3][34/100]\tLoss_D: 2.0693\tLoss_G: 0.9400\tD(x): 0.3824\tD(G(z)): 0.5485 / 0.4348\n",
      "g_grad:  tensor(1.4928e-05) d_grad tensor(-0.0122)\n",
      "G training 1, loss 0.5094285607337952, D(G(z)) 0.6067498922348022\n",
      "G training 2, loss 0.605948805809021, D(G(z)) 0.552522599697113\n",
      "G training 3, loss 0.5598269104957581, D(G(z)) 0.5775473713874817\n",
      "G training 4, loss 0.5110612511634827, D(G(z)) 0.6009176969528198\n",
      "\n",
      "\n",
      "[1/3][35/100]\tLoss_D: 1.4659\tLoss_G: 0.5111\tD(x): 0.5070\tD(G(z)): 0.5340 / 0.6009\n",
      "g_grad:  tensor(-1.0584e-06) d_grad tensor(-0.0121)\n",
      "G training 1, loss 0.5903497934341431, D(G(z)) 0.5565115809440613\n",
      "G training 2, loss 0.678051769733429, D(G(z)) 0.5155408382415771\n",
      "G training 3, loss 1.2907044887542725, D(G(z)) 0.28837621212005615\n",
      "G training 4, loss 0.7126527428627014, D(G(z)) 0.49937528371810913\n",
      "\n",
      "\n",
      "[1/3][36/100]\tLoss_D: 1.7040\tLoss_G: 0.7127\tD(x): 0.4982\tD(G(z)): 0.5817 / 0.4994\n",
      "g_grad:  tensor(0.0002) d_grad tensor(-0.0525)\n",
      "G training 1, loss 0.7410539984703064, D(G(z)) 0.4923354983329773\n",
      "G training 2, loss 0.6065804958343506, D(G(z)) 0.549576461315155\n",
      "G training 3, loss 0.648379921913147, D(G(z)) 0.5453388094902039\n",
      "G training 4, loss 0.6412093639373779, D(G(z)) 0.5362515449523926\n",
      "\n",
      "\n",
      "[1/3][37/100]\tLoss_D: 1.7478\tLoss_G: 0.6412\tD(x): 0.4413\tD(G(z)): 0.5975 / 0.5363\n",
      "g_grad:  tensor(0.0002) d_grad tensor(-0.0315)\n",
      "G training 1, loss 0.5018899440765381, D(G(z)) 0.6080969572067261\n",
      "G training 2, loss 0.707591712474823, D(G(z)) 0.4945147633552551\n",
      "G training 3, loss 0.825701892375946, D(G(z)) 0.4657253623008728\n",
      "G training 4, loss 0.6906108856201172, D(G(z)) 0.507313072681427\n",
      "\n",
      "\n",
      "[1/3][38/100]\tLoss_D: 3.0647\tLoss_G: 0.6906\tD(x): 0.1167\tD(G(z)): 0.4883 / 0.5073\n",
      "g_grad:  tensor(-5.3950e-05) d_grad tensor(-0.0204)\n",
      "G training 1, loss 0.5472671985626221, D(G(z)) 0.5837289690971375\n",
      "G training 2, loss 0.44544604420661926, D(G(z)) 0.6411713361740112\n",
      "G training 3, loss 0.4619238078594208, D(G(z)) 0.6362075209617615\n",
      "G training 4, loss 0.5010421276092529, D(G(z)) 0.6089069843292236\n",
      "\n",
      "\n",
      "[1/3][39/100]\tLoss_D: 1.7478\tLoss_G: 0.5010\tD(x): 0.4863\tD(G(z)): 0.6305 / 0.6089\n",
      "g_grad:  tensor(-6.9336e-06) d_grad tensor(-0.0253)\n",
      "G training 1, loss 0.573347270488739, D(G(z)) 0.5670663118362427\n",
      "G training 2, loss 0.8989406824111938, D(G(z)) 0.41935592889785767\n",
      "G training 3, loss 0.9978774189949036, D(G(z)) 0.38641995191574097\n",
      "G training 4, loss 0.4593319892883301, D(G(z)) 0.6337467432022095\n",
      "\n",
      "\n",
      "[1/3][40/100]\tLoss_D: 1.4178\tLoss_G: 0.4593\tD(x): 0.5212\tD(G(z)): 0.5194 / 0.6337\n",
      "g_grad:  tensor(1.9351e-05) d_grad tensor(-0.0075)\n",
      "G training 1, loss 0.39548760652542114, D(G(z)) 0.6746434569358826\n",
      "G training 2, loss 0.7152640223503113, D(G(z)) 0.49157583713531494\n",
      "G training 3, loss 0.4508790373802185, D(G(z)) 0.6382747292518616\n",
      "G training 4, loss 0.484702467918396, D(G(z)) 0.6188467741012573\n",
      "\n",
      "\n",
      "[1/3][41/100]\tLoss_D: 2.0671\tLoss_G: 0.4847\tD(x): 0.3148\tD(G(z)): 0.5728 / 0.6188\n",
      "g_grad:  tensor(0.0001) d_grad tensor(-0.0094)\n",
      "G training 1, loss 0.5521360635757446, D(G(z)) 0.5773721933364868\n",
      "G training 2, loss 0.6218887567520142, D(G(z)) 0.5399974584579468\n",
      "G training 3, loss 0.540888786315918, D(G(z)) 0.5843208432197571\n",
      "G training 4, loss 0.5080515146255493, D(G(z)) 0.6028449535369873\n",
      "\n",
      "\n",
      "[1/3][42/100]\tLoss_D: 1.7050\tLoss_G: 0.5081\tD(x): 0.5314\tD(G(z)): 0.6527 / 0.6028\n",
      "g_grad:  tensor(-0.0001) d_grad tensor(0.0018)\n",
      "G training 1, loss 0.7011277675628662, D(G(z)) 0.5000982284545898\n",
      "G training 2, loss 0.4234176278114319, D(G(z)) 0.6587033271789551\n",
      "G training 3, loss 0.4694415330886841, D(G(z)) 0.6260946393013\n",
      "G training 4, loss 0.46644753217697144, D(G(z)) 0.6289873123168945\n",
      "\n",
      "\n",
      "[1/3][43/100]\tLoss_D: 1.5775\tLoss_G: 0.4664\tD(x): 0.5006\tD(G(z)): 0.5820 / 0.6290\n",
      "g_grad:  tensor(-1.5732e-05) d_grad tensor(-0.0011)\n",
      "G training 1, loss 0.7621656060218811, D(G(z)) 0.4821234941482544\n",
      "G training 2, loss 0.7042628526687622, D(G(z)) 0.5116804838180542\n",
      "G training 3, loss 0.7574957013130188, D(G(z)) 0.48294588923454285\n",
      "G training 4, loss 0.7708598375320435, D(G(z)) 0.4756608009338379\n",
      "\n",
      "\n",
      "[1/3][44/100]\tLoss_D: 1.7503\tLoss_G: 0.7709\tD(x): 0.5077\tD(G(z)): 0.6464 / 0.4757\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0099)\n",
      "G training 1, loss 0.7494896650314331, D(G(z)) 0.48528534173965454\n",
      "G training 2, loss 0.7368105053901672, D(G(z)) 0.4920003116130829\n",
      "G training 3, loss 0.8660948276519775, D(G(z)) 0.4337909519672394\n",
      "G training 4, loss 0.6952213048934937, D(G(z)) 0.5255658030509949\n",
      "\n",
      "\n",
      "[1/3][45/100]\tLoss_D: 1.6032\tLoss_G: 0.6952\tD(x): 0.4558\tD(G(z)): 0.5498 / 0.5256\n",
      "g_grad:  tensor(5.9714e-05) d_grad tensor(0.0080)\n",
      "G training 1, loss 0.6307578682899475, D(G(z)) 0.5337772369384766\n",
      "G training 2, loss 0.6064627170562744, D(G(z)) 0.5542276501655579\n",
      "G training 3, loss 0.5367006659507751, D(G(z)) 0.585586667060852\n",
      "G training 4, loss 0.48145297169685364, D(G(z)) 0.6210032105445862\n",
      "\n",
      "\n",
      "[1/3][46/100]\tLoss_D: 1.8024\tLoss_G: 0.4815\tD(x): 0.3782\tD(G(z)): 0.5493 / 0.6210\n",
      "g_grad:  tensor(8.3285e-05) d_grad tensor(0.0126)\n",
      "G training 1, loss 0.5937241911888123, D(G(z)) 0.5531913638114929\n",
      "G training 2, loss 0.6319559812545776, D(G(z)) 0.5348507761955261\n",
      "G training 3, loss 0.6096525192260742, D(G(z)) 0.5498052835464478\n",
      "G training 4, loss 0.4996052086353302, D(G(z)) 0.6098644733428955\n",
      "\n",
      "\n",
      "[1/3][47/100]\tLoss_D: 2.1450\tLoss_G: 0.4996\tD(x): 0.2892\tD(G(z)): 0.5403 / 0.6099\n",
      "g_grad:  tensor(8.7118e-06) d_grad tensor(0.0061)\n",
      "G training 1, loss 0.5622339844703674, D(G(z)) 0.5768409967422485\n",
      "G training 2, loss 0.5926257967948914, D(G(z)) 0.5540406703948975\n",
      "G training 3, loss 0.4397108554840088, D(G(z)) 0.6494655609130859\n",
      "G training 4, loss 0.46512895822525024, D(G(z)) 0.6318216323852539\n",
      "\n",
      "\n",
      "[1/3][48/100]\tLoss_D: 1.8068\tLoss_G: 0.4651\tD(x): 0.4661\tD(G(z)): 0.6280 / 0.6318\n",
      "g_grad:  tensor(-4.1603e-05) d_grad tensor(0.0077)\n",
      "G training 1, loss 0.6747480630874634, D(G(z)) 0.5271197557449341\n",
      "G training 2, loss 0.5092172026634216, D(G(z)) 0.6142940521240234\n",
      "G training 3, loss 0.44294771552085876, D(G(z)) 0.6436468958854675\n",
      "G training 4, loss 0.4242250621318817, D(G(z)) 0.65641188621521\n",
      "\n",
      "\n",
      "[1/3][49/100]\tLoss_D: 1.7559\tLoss_G: 0.4242\tD(x): 0.5410\tD(G(z)): 0.6664 / 0.6564\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0065)\n",
      "G training 1, loss 0.6609112024307251, D(G(z)) 0.5303129553794861\n",
      "G training 2, loss 0.7002732753753662, D(G(z)) 0.5144533514976501\n",
      "G training 3, loss 0.599141001701355, D(G(z)) 0.5701216459274292\n",
      "G training 4, loss 0.5962549448013306, D(G(z)) 0.5554380416870117\n",
      "\n",
      "\n",
      "[1/3][50/100]\tLoss_D: 1.4561\tLoss_G: 0.5963\tD(x): 0.5956\tD(G(z)): 0.6053 / 0.5554\n",
      "g_grad:  tensor(-9.0473e-05) d_grad tensor(0.0223)\n",
      "G training 1, loss 0.49011337757110596, D(G(z)) 0.6150440573692322\n",
      "G training 2, loss 0.6904816627502441, D(G(z)) 0.5214501619338989\n",
      "G training 3, loss 0.5699478983879089, D(G(z)) 0.5721421837806702\n",
      "G training 4, loss 0.6465501189231873, D(G(z)) 0.532912015914917\n",
      "\n",
      "\n",
      "[1/3][51/100]\tLoss_D: 1.4066\tLoss_G: 0.6466\tD(x): 0.5501\tD(G(z)): 0.5420 / 0.5329\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(0.0171)\n",
      "G training 1, loss 0.6808371543884277, D(G(z)) 0.5100841522216797\n",
      "G training 2, loss 0.6216555237770081, D(G(z)) 0.545990526676178\n",
      "G training 3, loss 0.4796108603477478, D(G(z)) 0.6211678981781006\n",
      "G training 4, loss 0.5203703045845032, D(G(z)) 0.5957162976264954\n",
      "\n",
      "\n",
      "[1/3][52/100]\tLoss_D: 1.3629\tLoss_G: 0.5204\tD(x): 0.5335\tD(G(z)): 0.5116 / 0.5957\n",
      "g_grad:  tensor(0.0005) d_grad tensor(0.0133)\n",
      "G training 1, loss 0.5635520219802856, D(G(z)) 0.5707041025161743\n",
      "G training 2, loss 0.6415510773658752, D(G(z)) 0.5425044894218445\n",
      "G training 3, loss 0.4948578178882599, D(G(z)) 0.6247414350509644\n",
      "G training 4, loss 0.4914766848087311, D(G(z)) 0.6169826984405518\n",
      "\n",
      "\n",
      "[1/3][53/100]\tLoss_D: 1.7863\tLoss_G: 0.4915\tD(x): 0.5230\tD(G(z)): 0.6610 / 0.6170\n",
      "g_grad:  tensor(-0.0001) d_grad tensor(0.0055)\n",
      "G training 1, loss 0.4878525733947754, D(G(z)) 0.6223912239074707\n",
      "G training 2, loss 0.465076744556427, D(G(z)) 0.6303852796554565\n",
      "G training 3, loss 0.4634826183319092, D(G(z)) 0.6337566375732422\n",
      "G training 4, loss 0.48521876335144043, D(G(z)) 0.6185988187789917\n",
      "\n",
      "\n",
      "[1/3][54/100]\tLoss_D: 1.9103\tLoss_G: 0.4852\tD(x): 0.4407\tD(G(z)): 0.6549 / 0.6186\n",
      "g_grad:  tensor(-0.0001) d_grad tensor(-0.0001)\n",
      "G training 1, loss 0.5372129082679749, D(G(z)) 0.5896750092506409\n",
      "G training 2, loss 0.5016189813613892, D(G(z)) 0.6110309958457947\n",
      "G training 3, loss 0.3424106240272522, D(G(z)) 0.7180570960044861\n",
      "G training 4, loss 0.39198124408721924, D(G(z)) 0.678862452507019\n",
      "\n",
      "\n",
      "[1/3][55/100]\tLoss_D: 1.6669\tLoss_G: 0.3920\tD(x): 0.4630\tD(G(z)): 0.5564 / 0.6789\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(-0.0021)\n",
      "G training 1, loss 0.5415294766426086, D(G(z)) 0.582184374332428\n",
      "G training 2, loss 0.5523582696914673, D(G(z)) 0.5782882571220398\n",
      "G training 3, loss 0.6288473606109619, D(G(z)) 0.5376465320587158\n",
      "G training 4, loss 0.5032222867012024, D(G(z)) 0.6076669692993164\n",
      "\n",
      "\n",
      "[1/3][56/100]\tLoss_D: 2.4528\tLoss_G: 0.5032\tD(x): 0.3815\tD(G(z)): 0.7193 / 0.6077\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0043)\n",
      "G training 1, loss 0.7132935523986816, D(G(z)) 0.5031632781028748\n",
      "G training 2, loss 0.8326401114463806, D(G(z)) 0.44054868817329407\n",
      "G training 3, loss 0.6333173513412476, D(G(z)) 0.5326701998710632\n",
      "G training 4, loss 0.7175614833831787, D(G(z)) 0.4950195252895355\n",
      "\n",
      "\n",
      "[1/3][57/100]\tLoss_D: 1.6181\tLoss_G: 0.7176\tD(x): 0.5545\tD(G(z)): 0.6206 / 0.4950\n",
      "g_grad:  tensor(0.0009) d_grad tensor(-0.0028)\n",
      "G training 1, loss 0.6393800973892212, D(G(z)) 0.5291443467140198\n",
      "G training 2, loss 0.6096590161323547, D(G(z)) 0.5460485219955444\n",
      "G training 3, loss 0.5071972012519836, D(G(z)) 0.6056016087532043\n",
      "G training 4, loss 0.4396071434020996, D(G(z)) 0.6473307609558105\n",
      "\n",
      "\n",
      "[1/3][58/100]\tLoss_D: 2.1718\tLoss_G: 0.4396\tD(x): 0.2849\tD(G(z)): 0.5773 / 0.6473\n",
      "g_grad:  tensor(3.8907e-05) d_grad tensor(-0.0311)\n",
      "G training 1, loss 0.5698933601379395, D(G(z)) 0.5671720504760742\n",
      "G training 2, loss 0.5497214794158936, D(G(z)) 0.5782985687255859\n",
      "G training 3, loss 0.4528699517250061, D(G(z)) 0.6367165446281433\n",
      "G training 4, loss 0.42486414313316345, D(G(z)) 0.6564401984214783\n",
      "\n",
      "\n",
      "[1/3][59/100]\tLoss_D: 2.9263\tLoss_G: 0.4249\tD(x): 0.2658\tD(G(z)): 0.6452 / 0.6564\n",
      "g_grad:  tensor(8.0507e-05) d_grad tensor(-0.0283)\n",
      "G training 1, loss 0.6391362547874451, D(G(z)) 0.5316897034645081\n",
      "G training 2, loss 0.5021619200706482, D(G(z)) 0.6057907342910767\n",
      "G training 3, loss 0.5027495622634888, D(G(z)) 0.6052666306495667\n",
      "G training 4, loss 0.5275604128837585, D(G(z)) 0.591625988483429\n",
      "\n",
      "\n",
      "[1/3][60/100]\tLoss_D: 2.0228\tLoss_G: 0.5276\tD(x): 0.4375\tD(G(z)): 0.6873 / 0.5916\n",
      "g_grad:  tensor(8.1758e-05) d_grad tensor(-0.0321)\n",
      "G training 1, loss 0.7285981178283691, D(G(z)) 0.4833163619041443\n",
      "G training 2, loss 0.7733303904533386, D(G(z)) 0.4705164134502411\n",
      "G training 3, loss 0.8880616426467896, D(G(z)) 0.43112534284591675\n",
      "G training 4, loss 0.5477381944656372, D(G(z)) 0.5783933401107788\n",
      "\n",
      "\n",
      "[1/3][61/100]\tLoss_D: 1.6082\tLoss_G: 0.5477\tD(x): 0.5517\tD(G(z)): 0.6333 / 0.5784\n",
      "g_grad:  tensor(-3.1360e-05) d_grad tensor(-0.0221)\n",
      "G training 1, loss 0.7735168933868408, D(G(z)) 0.46226587891578674\n",
      "G training 2, loss 0.7946250438690186, D(G(z)) 0.458980530500412\n",
      "G training 3, loss 0.7791367769241333, D(G(z)) 0.46330535411834717\n",
      "G training 4, loss 0.6214379668235779, D(G(z)) 0.5380910634994507\n",
      "\n",
      "\n",
      "[1/3][62/100]\tLoss_D: 1.7036\tLoss_G: 0.6214\tD(x): 0.4066\tD(G(z)): 0.5319 / 0.5381\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(-0.0329)\n",
      "G training 1, loss 0.7706393003463745, D(G(z)) 0.46839678287506104\n",
      "G training 2, loss 0.6792547702789307, D(G(z)) 0.5103427171707153\n",
      "G training 3, loss 0.5267712473869324, D(G(z)) 0.5949310064315796\n",
      "G training 4, loss 0.49760526418685913, D(G(z)) 0.6086758971214294\n",
      "\n",
      "\n",
      "[1/3][63/100]\tLoss_D: 1.3811\tLoss_G: 0.4976\tD(x): 0.5528\tD(G(z)): 0.5405 / 0.6087\n",
      "g_grad:  tensor(-0.0007) d_grad tensor(-0.0291)\n",
      "G training 1, loss 0.5572463274002075, D(G(z)) 0.5732645392417908\n",
      "G training 2, loss 0.5881301164627075, D(G(z)) 0.5564330220222473\n",
      "G training 3, loss 0.4403308033943176, D(G(z)) 0.6470145583152771\n",
      "G training 4, loss 0.4675658941268921, D(G(z)) 0.6271260976791382\n",
      "\n",
      "\n",
      "[1/3][64/100]\tLoss_D: 1.9405\tLoss_G: 0.4676\tD(x): 0.4263\tD(G(z)): 0.6483 / 0.6271\n",
      "g_grad:  tensor(0.0006) d_grad tensor(-0.0038)\n",
      "G training 1, loss 0.7227391600608826, D(G(z)) 0.49228018522262573\n",
      "G training 2, loss 0.7065460681915283, D(G(z)) 0.518596351146698\n",
      "G training 3, loss 0.6023415923118591, D(G(z)) 0.5549538731575012\n",
      "G training 4, loss 0.4652450978755951, D(G(z)) 0.6324761509895325\n",
      "\n",
      "\n",
      "[1/3][65/100]\tLoss_D: 2.6157\tLoss_G: 0.4652\tD(x): 0.2664\tD(G(z)): 0.6816 / 0.6325\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0159)\n",
      "G training 1, loss 0.6436405777931213, D(G(z)) 0.5330461859703064\n",
      "G training 2, loss 0.6682819724082947, D(G(z)) 0.5189896821975708\n",
      "G training 3, loss 0.6035113334655762, D(G(z)) 0.5490425825119019\n",
      "G training 4, loss 0.5516034960746765, D(G(z)) 0.5807262659072876\n",
      "\n",
      "\n",
      "[1/3][66/100]\tLoss_D: 3.2999\tLoss_G: 0.5516\tD(x): 0.1255\tD(G(z)): 0.6348 / 0.5807\n",
      "g_grad:  tensor(0.0008) d_grad tensor(0.0176)\n",
      "G training 1, loss 0.4795497953891754, D(G(z)) 0.6256351470947266\n",
      "G training 2, loss 0.8563506603240967, D(G(z)) 0.46820318698883057\n",
      "G training 3, loss 0.40688449144363403, D(G(z)) 0.6705235242843628\n",
      "G training 4, loss 0.44053784012794495, D(G(z)) 0.6480227112770081\n",
      "\n",
      "\n",
      "[1/3][67/100]\tLoss_D: 2.1895\tLoss_G: 0.4405\tD(x): 0.2865\tD(G(z)): 0.5768 / 0.6480\n",
      "g_grad:  tensor(0.0004) d_grad tensor(-0.0064)\n",
      "G training 1, loss 0.570658802986145, D(G(z)) 0.5676566362380981\n",
      "G training 2, loss 0.5763767957687378, D(G(z)) 0.5660239458084106\n",
      "G training 3, loss 0.4705989956855774, D(G(z)) 0.6253425478935242\n",
      "G training 4, loss 0.5610310435295105, D(G(z)) 0.5755703449249268\n",
      "\n",
      "\n",
      "[1/3][68/100]\tLoss_D: 2.0130\tLoss_G: 0.5610\tD(x): 0.3900\tD(G(z)): 0.6221 / 0.5756\n",
      "g_grad:  tensor(2.6037e-05) d_grad tensor(-0.0072)\n",
      "G training 1, loss 0.7403879165649414, D(G(z)) 0.4857732653617859\n",
      "G training 2, loss 0.6041452288627625, D(G(z)) 0.5486740469932556\n",
      "G training 3, loss 0.6208711862564087, D(G(z)) 0.5396443009376526\n",
      "G training 4, loss 0.5228599309921265, D(G(z)) 0.594696581363678\n",
      "\n",
      "\n",
      "[1/3][69/100]\tLoss_D: 1.5884\tLoss_G: 0.5229\tD(x): 0.6378\tD(G(z)): 0.6767 / 0.5947\n",
      "g_grad:  tensor(4.4400e-05) d_grad tensor(-0.0068)\n",
      "G training 1, loss 0.5902710556983948, D(G(z)) 0.5552091598510742\n",
      "G training 2, loss 0.9006435871124268, D(G(z)) 0.43346574902534485\n",
      "G training 3, loss 0.6081296801567078, D(G(z)) 0.546445369720459\n",
      "G training 4, loss 0.5112398862838745, D(G(z)) 0.6034030914306641\n",
      "\n",
      "\n",
      "[1/3][70/100]\tLoss_D: 1.7875\tLoss_G: 0.5112\tD(x): 0.4536\tD(G(z)): 0.6154 / 0.6034\n",
      "g_grad:  tensor(7.3906e-05) d_grad tensor(0.0073)\n",
      "G training 1, loss 0.474082887172699, D(G(z)) 0.6242471933364868\n",
      "G training 2, loss 0.6436476707458496, D(G(z)) 0.5290014147758484\n",
      "G training 3, loss 0.6386594176292419, D(G(z)) 0.5377389788627625\n",
      "G training 4, loss 0.5026012659072876, D(G(z)) 0.6099047660827637\n",
      "\n",
      "\n",
      "[1/3][71/100]\tLoss_D: 1.7468\tLoss_G: 0.5026\tD(x): 0.4755\tD(G(z)): 0.6224 / 0.6099\n",
      "g_grad:  tensor(-4.8958e-05) d_grad tensor(-0.0068)\n",
      "G training 1, loss 0.6718724370002747, D(G(z)) 0.5128629207611084\n",
      "G training 2, loss 0.6160467863082886, D(G(z)) 0.5426139235496521\n",
      "G training 3, loss 0.6518456339836121, D(G(z)) 0.5290529131889343\n",
      "G training 4, loss 0.6235863566398621, D(G(z)) 0.5518553853034973\n",
      "\n",
      "\n",
      "[1/3][72/100]\tLoss_D: 1.6604\tLoss_G: 0.6236\tD(x): 0.5091\tD(G(z)): 0.6187 / 0.5519\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(-0.0043)\n",
      "G training 1, loss 0.6794958114624023, D(G(z)) 0.5109311938285828\n",
      "G training 2, loss 0.6589144468307495, D(G(z)) 0.5201407074928284\n",
      "G training 3, loss 0.6220492720603943, D(G(z)) 0.5385883450508118\n",
      "G training 4, loss 0.6111156344413757, D(G(z)) 0.544404149055481\n",
      "\n",
      "\n",
      "[1/3][73/100]\tLoss_D: 1.5967\tLoss_G: 0.6111\tD(x): 0.4939\tD(G(z)): 0.5751 / 0.5444\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(-0.0043)\n",
      "G training 1, loss 0.6416878700256348, D(G(z)) 0.5324226021766663\n",
      "G training 2, loss 0.6312355399131775, D(G(z)) 0.5350735187530518\n",
      "G training 3, loss 0.5758999586105347, D(G(z)) 0.5643789768218994\n",
      "G training 4, loss 0.6672844886779785, D(G(z)) 0.5163450837135315\n",
      "\n",
      "\n",
      "[1/3][74/100]\tLoss_D: 1.6621\tLoss_G: 0.6673\tD(x): 0.4243\tD(G(z)): 0.5434 / 0.5163\n",
      "g_grad:  tensor(0.0001) d_grad tensor(-0.0086)\n",
      "G training 1, loss 0.5666264295578003, D(G(z)) 0.5713242292404175\n",
      "G training 2, loss 0.569345235824585, D(G(z)) 0.5708053708076477\n",
      "G training 3, loss 1.033860445022583, D(G(z)) 0.3666194677352905\n",
      "G training 4, loss 0.6320244669914246, D(G(z)) 0.5428359508514404\n",
      "\n",
      "\n",
      "[1/3][75/100]\tLoss_D: 1.7295\tLoss_G: 0.6320\tD(x): 0.3738\tD(G(z)): 0.5165 / 0.5428\n",
      "g_grad:  tensor(-7.6257e-05) d_grad tensor(-0.0124)\n",
      "G training 1, loss 0.6327314376831055, D(G(z)) 0.538374662399292\n",
      "G training 2, loss 0.5454482436180115, D(G(z)) 0.5808083415031433\n",
      "G training 3, loss 0.8458375930786133, D(G(z)) 0.4538470506668091\n",
      "G training 4, loss 0.4785929322242737, D(G(z)) 0.622711718082428\n",
      "\n",
      "\n",
      "[1/3][76/100]\tLoss_D: 1.7925\tLoss_G: 0.4786\tD(x): 0.4162\tD(G(z)): 0.5435 / 0.6227\n",
      "g_grad:  tensor(1.1987e-05) d_grad tensor(0.0017)\n",
      "G training 1, loss 0.6394659876823425, D(G(z)) 0.5441206097602844\n",
      "G training 2, loss 0.6232744455337524, D(G(z)) 0.554826557636261\n",
      "G training 3, loss 0.6620405316352844, D(G(z)) 0.5257057547569275\n",
      "G training 4, loss 0.4771052896976471, D(G(z)) 0.6239980459213257\n",
      "\n",
      "\n",
      "[1/3][77/100]\tLoss_D: 2.0493\tLoss_G: 0.4771\tD(x): 0.3364\tD(G(z)): 0.5839 / 0.6240\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(-0.0097)\n",
      "G training 1, loss 0.5800980925559998, D(G(z)) 0.5671725273132324\n",
      "G training 2, loss 0.5776389837265015, D(G(z)) 0.5643929839134216\n",
      "G training 3, loss 0.5105871558189392, D(G(z)) 0.6027055382728577\n",
      "G training 4, loss 0.45588186383247375, D(G(z)) 0.6384006142616272\n",
      "\n",
      "\n",
      "[1/3][78/100]\tLoss_D: 1.6056\tLoss_G: 0.4559\tD(x): 0.4991\tD(G(z)): 0.5862 / 0.6384\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(-0.0081)\n",
      "G training 1, loss 0.5336195826530457, D(G(z)) 0.5897480845451355\n",
      "G training 2, loss 0.4802098572254181, D(G(z)) 0.6231810450553894\n",
      "G training 3, loss 0.4830637574195862, D(G(z)) 0.6175055503845215\n",
      "G training 4, loss 0.541893482208252, D(G(z)) 0.584099531173706\n",
      "\n",
      "\n",
      "[1/3][79/100]\tLoss_D: 1.7449\tLoss_G: 0.5419\tD(x): 0.4909\tD(G(z)): 0.6361 / 0.5841\n",
      "g_grad:  tensor(-3.1478e-05) d_grad tensor(0.0020)\n",
      "G training 1, loss 0.6600985527038574, D(G(z)) 0.5202639102935791\n",
      "G training 2, loss 0.4813581109046936, D(G(z)) 0.618540346622467\n",
      "G training 3, loss 0.5733377933502197, D(G(z)) 0.5682834386825562\n",
      "G training 4, loss 0.5972115993499756, D(G(z)) 0.5580136775970459\n",
      "\n",
      "\n",
      "[1/3][80/100]\tLoss_D: 1.7929\tLoss_G: 0.5972\tD(x): 0.5036\tD(G(z)): 0.6644 / 0.5580\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0039)\n",
      "G training 1, loss 0.6587492227554321, D(G(z)) 0.5236359238624573\n",
      "G training 2, loss 0.598876953125, D(G(z)) 0.5551584959030151\n",
      "G training 3, loss 0.7983309626579285, D(G(z)) 0.4732280373573303\n",
      "G training 4, loss 0.4448176920413971, D(G(z)) 0.6443064212799072\n",
      "\n",
      "\n",
      "[1/3][81/100]\tLoss_D: 1.4590\tLoss_G: 0.4448\tD(x): 0.5590\tD(G(z)): 0.5726 / 0.6443\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0141)\n",
      "G training 1, loss 0.7530124187469482, D(G(z)) 0.47989246249198914\n",
      "G training 2, loss 0.5691263675689697, D(G(z)) 0.5693188905715942\n",
      "G training 3, loss 0.7606832385063171, D(G(z)) 0.47877615690231323\n",
      "G training 4, loss 0.5839722156524658, D(G(z)) 0.561607837677002\n",
      "\n",
      "\n",
      "[1/3][82/100]\tLoss_D: 1.6094\tLoss_G: 0.5840\tD(x): 0.5332\tD(G(z)): 0.6213 / 0.5616\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0294)\n",
      "G training 1, loss 0.7707687020301819, D(G(z)) 0.4653245508670807\n",
      "G training 2, loss 0.7267112731933594, D(G(z)) 0.4878392815589905\n",
      "G training 3, loss 0.603820264339447, D(G(z)) 0.5505483150482178\n",
      "G training 4, loss 0.731149435043335, D(G(z)) 0.5041612982749939\n",
      "\n",
      "\n",
      "[1/3][83/100]\tLoss_D: 1.6464\tLoss_G: 0.7311\tD(x): 0.5237\tD(G(z)): 0.6247 / 0.5042\n",
      "g_grad:  tensor(2.9866e-05) d_grad tensor(0.0463)\n",
      "G training 1, loss 0.9773267507553101, D(G(z)) 0.3868539035320282\n",
      "G training 2, loss 0.6771184206008911, D(G(z)) 0.5088908672332764\n",
      "G training 3, loss 0.6957964301109314, D(G(z)) 0.5084915161132812\n",
      "G training 4, loss 0.6082921624183655, D(G(z)) 0.5549071431159973\n",
      "\n",
      "\n",
      "[1/3][84/100]\tLoss_D: 1.7346\tLoss_G: 0.6083\tD(x): 0.4432\tD(G(z)): 0.5885 / 0.5549\n",
      "g_grad:  tensor(2.0871e-06) d_grad tensor(0.0472)\n",
      "G training 1, loss 0.6505942940711975, D(G(z)) 0.5226766467094421\n",
      "G training 2, loss 0.8981789946556091, D(G(z)) 0.4648860692977905\n",
      "G training 3, loss 0.7060118317604065, D(G(z)) 0.4984476864337921\n",
      "G training 4, loss 0.677204966545105, D(G(z)) 0.512374758720398\n",
      "\n",
      "\n",
      "[1/3][85/100]\tLoss_D: 1.7092\tLoss_G: 0.6772\tD(x): 0.4631\tD(G(z)): 0.5931 / 0.5124\n",
      "g_grad:  tensor(-3.1709e-05) d_grad tensor(0.0393)\n",
      "G training 1, loss 0.7406480312347412, D(G(z)) 0.48996734619140625\n",
      "G training 2, loss 0.7079423069953918, D(G(z)) 0.5091531276702881\n",
      "G training 3, loss 0.8479332327842712, D(G(z)) 0.4511675536632538\n",
      "G training 4, loss 0.5997592210769653, D(G(z)) 0.553249180316925\n",
      "\n",
      "\n",
      "[1/3][86/100]\tLoss_D: 1.6683\tLoss_G: 0.5998\tD(x): 0.4491\tD(G(z)): 0.5555 / 0.5532\n",
      "g_grad:  tensor(3.7127e-05) d_grad tensor(0.0185)\n",
      "G training 1, loss 0.7220208644866943, D(G(z)) 0.491626501083374\n",
      "G training 2, loss 0.6019327640533447, D(G(z)) 0.5487961769104004\n",
      "G training 3, loss 0.7442430257797241, D(G(z)) 0.4930967688560486\n",
      "G training 4, loss 0.6237750053405762, D(G(z)) 0.5400002598762512\n",
      "\n",
      "\n",
      "[1/3][87/100]\tLoss_D: 1.7846\tLoss_G: 0.6238\tD(x): 0.4413\tD(G(z)): 0.5925 / 0.5400\n",
      "g_grad:  tensor(7.8658e-05) d_grad tensor(0.0349)\n",
      "G training 1, loss 0.6819537878036499, D(G(z)) 0.5114874839782715\n",
      "G training 2, loss 0.7265897989273071, D(G(z)) 0.4860554337501526\n",
      "G training 3, loss 0.5338422060012817, D(G(z)) 0.5866549015045166\n",
      "G training 4, loss 0.6057231426239014, D(G(z)) 0.56145179271698\n",
      "\n",
      "\n",
      "[1/3][88/100]\tLoss_D: 1.7192\tLoss_G: 0.6057\tD(x): 0.4527\tD(G(z)): 0.5862 / 0.5615\n",
      "g_grad:  tensor(2.0467e-05) d_grad tensor(0.0350)\n",
      "G training 1, loss 0.8607843518257141, D(G(z)) 0.42936575412750244\n",
      "G training 2, loss 0.6068863868713379, D(G(z)) 0.5466732382774353\n",
      "G training 3, loss 0.6695366501808167, D(G(z)) 0.518425464630127\n",
      "G training 4, loss 0.6390030980110168, D(G(z)) 0.5311149954795837\n",
      "\n",
      "\n",
      "[1/3][89/100]\tLoss_D: 1.9694\tLoss_G: 0.6390\tD(x): 0.3854\tD(G(z)): 0.6023 / 0.5311\n",
      "g_grad:  tensor(0.0004) d_grad tensor(0.0111)\n",
      "G training 1, loss 0.6931259036064148, D(G(z)) 0.5150614976882935\n",
      "G training 2, loss 0.6237660646438599, D(G(z)) 0.5367014408111572\n",
      "G training 3, loss 0.6224678754806519, D(G(z)) 0.5399343371391296\n",
      "G training 4, loss 0.5177485346794128, D(G(z)) 0.6006730794906616\n",
      "\n",
      "\n",
      "[1/3][90/100]\tLoss_D: 1.6324\tLoss_G: 0.5177\tD(x): 0.4472\tD(G(z)): 0.5462 / 0.6007\n",
      "g_grad:  tensor(-3.6152e-05) d_grad tensor(0.0154)\n",
      "G training 1, loss 0.9573159217834473, D(G(z)) 0.39009034633636475\n",
      "G training 2, loss 1.1344215869903564, D(G(z)) 0.32974672317504883\n",
      "G training 3, loss 0.77700275182724, D(G(z)) 0.4669661819934845\n",
      "G training 4, loss 0.8501496315002441, D(G(z)) 0.44204622507095337\n",
      "\n",
      "\n",
      "[1/3][91/100]\tLoss_D: 1.6277\tLoss_G: 0.8501\tD(x): 0.5314\tD(G(z)): 0.5952 / 0.4420\n",
      "g_grad:  tensor(6.1203e-05) d_grad tensor(0.0217)\n",
      "G training 1, loss 0.836952805519104, D(G(z)) 0.44224828481674194\n",
      "G training 2, loss 0.749971330165863, D(G(z)) 0.47509798407554626\n",
      "G training 3, loss 0.7214862108230591, D(G(z)) 0.4882759153842926\n",
      "G training 4, loss 0.6789764165878296, D(G(z)) 0.5110533237457275\n",
      "\n",
      "\n",
      "[1/3][92/100]\tLoss_D: 1.3704\tLoss_G: 0.6790\tD(x): 0.5167\tD(G(z)): 0.4779 / 0.5111\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0124)\n",
      "G training 1, loss 0.6843907833099365, D(G(z)) 0.5107719302177429\n",
      "G training 2, loss 0.7385300397872925, D(G(z)) 0.48690494894981384\n",
      "G training 3, loss 0.7690570950508118, D(G(z)) 0.4696407616138458\n",
      "G training 4, loss 0.8052965998649597, D(G(z)) 0.4689173698425293\n",
      "\n",
      "\n",
      "[1/3][93/100]\tLoss_D: 1.4209\tLoss_G: 0.8053\tD(x): 0.5481\tD(G(z)): 0.5527 / 0.4689\n",
      "g_grad:  tensor(-4.7563e-05) d_grad tensor(0.0145)\n",
      "G training 1, loss 0.7172603607177734, D(G(z)) 0.4914094805717468\n",
      "G training 2, loss 0.8318376541137695, D(G(z)) 0.44074106216430664\n",
      "G training 3, loss 0.6248467564582825, D(G(z)) 0.5408281087875366\n",
      "G training 4, loss 0.729569673538208, D(G(z)) 0.48396825790405273\n",
      "\n",
      "\n",
      "[1/3][94/100]\tLoss_D: 1.4643\tLoss_G: 0.7296\tD(x): 0.4665\tD(G(z)): 0.4660 / 0.4840\n",
      "g_grad:  tensor(9.0353e-06) d_grad tensor(0.0137)\n",
      "G training 1, loss 0.8012292981147766, D(G(z)) 0.45294415950775146\n",
      "G training 2, loss 0.7097718119621277, D(G(z)) 0.49362486600875854\n",
      "G training 3, loss 0.716750979423523, D(G(z)) 0.4901224672794342\n",
      "G training 4, loss 0.5941950082778931, D(G(z)) 0.5557743906974792\n",
      "\n",
      "\n",
      "[1/3][95/100]\tLoss_D: 1.6946\tLoss_G: 0.5942\tD(x): 0.4269\tD(G(z)): 0.5422 / 0.5558\n",
      "g_grad:  tensor(2.7101e-05) d_grad tensor(0.0112)\n",
      "G training 1, loss 0.7050997614860535, D(G(z)) 0.49848875403404236\n",
      "G training 2, loss 0.6380317807197571, D(G(z)) 0.5325058102607727\n",
      "G training 3, loss 0.6684890985488892, D(G(z)) 0.517306387424469\n",
      "G training 4, loss 0.5649879574775696, D(G(z)) 0.5736537575721741\n",
      "\n",
      "\n",
      "[1/3][96/100]\tLoss_D: 1.7326\tLoss_G: 0.5650\tD(x): 0.3678\tD(G(z)): 0.4874 / 0.5737\n",
      "g_grad:  tensor(-6.8472e-07) d_grad tensor(0.0125)\n",
      "G training 1, loss 0.6234361529350281, D(G(z)) 0.5378384590148926\n",
      "G training 2, loss 0.6481326222419739, D(G(z)) 0.5268863439559937\n",
      "G training 3, loss 0.5724673867225647, D(G(z)) 0.568040132522583\n",
      "G training 4, loss 0.4869788587093353, D(G(z)) 0.6163392663002014\n",
      "\n",
      "\n",
      "[1/3][97/100]\tLoss_D: 1.6126\tLoss_G: 0.4870\tD(x): 0.4704\tD(G(z)): 0.5491 / 0.6163\n",
      "g_grad:  tensor(4.1235e-05) d_grad tensor(0.0098)\n",
      "G training 1, loss 0.5768728256225586, D(G(z)) 0.56778484582901\n",
      "G training 2, loss 0.5836488604545593, D(G(z)) 0.563404381275177\n",
      "G training 3, loss 0.5719035267829895, D(G(z)) 0.5668485760688782\n",
      "G training 4, loss 0.7470804452896118, D(G(z)) 0.4987313151359558\n",
      "\n",
      "\n",
      "[1/3][98/100]\tLoss_D: 1.5992\tLoss_G: 0.7471\tD(x): 0.5447\tD(G(z)): 0.6151 / 0.4987\n",
      "g_grad:  tensor(-8.4920e-05) d_grad tensor(0.0050)\n",
      "G training 1, loss 0.7040340900421143, D(G(z)) 0.49516770243644714\n",
      "G training 2, loss 0.6949050426483154, D(G(z)) 0.5055720806121826\n",
      "G training 3, loss 0.7757740616798401, D(G(z)) 0.4690245985984802\n",
      "G training 4, loss 0.6568537950515747, D(G(z)) 0.5191563367843628\n",
      "\n",
      "\n",
      "[1/3][99/100]\tLoss_D: 1.3528\tLoss_G: 0.6569\tD(x): 0.6164\tD(G(z)): 0.5614 / 0.5192\n",
      "g_grad:  tensor(2.1559e-05) d_grad tensor(0.0061)\n",
      "G training 1, loss 0.8054852485656738, D(G(z)) 0.45833805203437805\n",
      "G training 2, loss 0.6921285390853882, D(G(z)) 0.507375180721283\n",
      "G training 3, loss 0.6781191825866699, D(G(z)) 0.5095126032829285\n",
      "G training 4, loss 0.6341999769210815, D(G(z)) 0.5307818055152893\n",
      "\n",
      "\n",
      "[2/3][0/100]\tLoss_D: 1.5255\tLoss_G: 0.6342\tD(x): 0.4878\tD(G(z)): 0.5469 / 0.5308\n",
      "g_grad:  tensor(-1.3229e-06) d_grad tensor(0.0059)\n",
      "G training 1, loss 0.7433137893676758, D(G(z)) 0.47916504740715027\n",
      "G training 2, loss 0.7039449214935303, D(G(z)) 0.4974663257598877\n",
      "G training 3, loss 0.7212212085723877, D(G(z)) 0.49227726459503174\n",
      "G training 4, loss 0.649599552154541, D(G(z)) 0.5257624387741089\n",
      "\n",
      "\n",
      "[2/3][1/100]\tLoss_D: 1.4310\tLoss_G: 0.6496\tD(x): 0.5106\tD(G(z)): 0.5235 / 0.5258\n",
      "g_grad:  tensor(9.2139e-05) d_grad tensor(0.0148)\n",
      "G training 1, loss 0.8502941131591797, D(G(z)) 0.4354686141014099\n",
      "G training 2, loss 0.7292183041572571, D(G(z)) 0.4829963147640228\n",
      "G training 3, loss 0.7965996861457825, D(G(z)) 0.4601806700229645\n",
      "G training 4, loss 0.6339507102966309, D(G(z)) 0.5356907248497009\n",
      "\n",
      "\n",
      "[2/3][2/100]\tLoss_D: 1.6061\tLoss_G: 0.6340\tD(x): 0.4891\tD(G(z)): 0.5675 / 0.5357\n",
      "g_grad:  tensor(4.5476e-05) d_grad tensor(0.0151)\n",
      "G training 1, loss 0.7640173435211182, D(G(z)) 0.46894219517707825\n",
      "G training 2, loss 0.9291326999664307, D(G(z)) 0.4044913053512573\n",
      "G training 3, loss 0.8366184234619141, D(G(z)) 0.4419323205947876\n",
      "G training 4, loss 0.7533674240112305, D(G(z)) 0.47686517238616943\n",
      "\n",
      "\n",
      "[2/3][3/100]\tLoss_D: 1.4931\tLoss_G: 0.7534\tD(x): 0.4845\tD(G(z)): 0.5133 / 0.4769\n",
      "g_grad:  tensor(-2.3655e-05) d_grad tensor(0.0299)\n",
      "G training 1, loss 0.7641147971153259, D(G(z)) 0.4695809483528137\n",
      "G training 2, loss 0.8471153974533081, D(G(z)) 0.43216538429260254\n",
      "G training 3, loss 0.6478596925735474, D(G(z)) 0.52508944272995\n",
      "G training 4, loss 0.718700110912323, D(G(z)) 0.49233442544937134\n",
      "\n",
      "\n",
      "[2/3][4/100]\tLoss_D: 1.3856\tLoss_G: 0.7187\tD(x): 0.5339\tD(G(z)): 0.5274 / 0.4923\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0190)\n",
      "G training 1, loss 0.799675464630127, D(G(z)) 0.4530867338180542\n",
      "G training 2, loss 0.7387709617614746, D(G(z)) 0.47877663373947144\n",
      "G training 3, loss 0.6400317549705505, D(G(z)) 0.5299915075302124\n",
      "G training 4, loss 0.7508518099784851, D(G(z)) 0.4783529043197632\n",
      "\n",
      "\n",
      "[2/3][5/100]\tLoss_D: 1.5718\tLoss_G: 0.7509\tD(x): 0.4708\tD(G(z)): 0.5511 / 0.4784\n",
      "g_grad:  tensor(-1.3388e-07) d_grad tensor(0.0250)\n",
      "G training 1, loss 0.7947728633880615, D(G(z)) 0.4555637836456299\n",
      "G training 2, loss 0.6810020208358765, D(G(z)) 0.5070209503173828\n",
      "G training 3, loss 0.6538508534431458, D(G(z)) 0.5205636024475098\n",
      "G training 4, loss 0.6526316404342651, D(G(z)) 0.5224406123161316\n",
      "\n",
      "\n",
      "[2/3][6/100]\tLoss_D: 1.3512\tLoss_G: 0.6526\tD(x): 0.5804\tD(G(z)): 0.5489 / 0.5224\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0153)\n",
      "G training 1, loss 0.8526095747947693, D(G(z)) 0.4291274845600128\n",
      "G training 2, loss 0.7027793526649475, D(G(z)) 0.49555984139442444\n",
      "G training 3, loss 0.797579824924469, D(G(z)) 0.4593609571456909\n",
      "G training 4, loss 0.5830729007720947, D(G(z)) 0.5605983138084412\n",
      "\n",
      "\n",
      "[2/3][7/100]\tLoss_D: 1.5433\tLoss_G: 0.5831\tD(x): 0.4763\tD(G(z)): 0.5430 / 0.5606\n",
      "g_grad:  tensor(3.4563e-05) d_grad tensor(0.0073)\n",
      "G training 1, loss 0.7554873824119568, D(G(z)) 0.4713086485862732\n",
      "G training 2, loss 0.6888245344161987, D(G(z)) 0.5026053786277771\n",
      "G training 3, loss 0.7538695335388184, D(G(z)) 0.473660409450531\n",
      "G training 4, loss 0.719876766204834, D(G(z)) 0.48724040389060974\n",
      "\n",
      "\n",
      "[2/3][8/100]\tLoss_D: 1.6206\tLoss_G: 0.7199\tD(x): 0.4415\tD(G(z)): 0.5389 / 0.4872\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0189)\n",
      "G training 1, loss 0.6502822041511536, D(G(z)) 0.5233809351921082\n",
      "G training 2, loss 0.6257539987564087, D(G(z)) 0.5374780297279358\n",
      "G training 3, loss 0.6242678761482239, D(G(z)) 0.5390221476554871\n",
      "G training 4, loss 0.7479248046875, D(G(z)) 0.47956743836402893\n",
      "\n",
      "\n",
      "[2/3][9/100]\tLoss_D: 2.0603\tLoss_G: 0.7479\tD(x): 0.3082\tD(G(z)): 0.5210 / 0.4796\n",
      "g_grad:  tensor(3.8844e-05) d_grad tensor(0.0266)\n",
      "G training 1, loss 0.7164992094039917, D(G(z)) 0.4962337017059326\n",
      "G training 2, loss 0.617358386516571, D(G(z)) 0.5440133810043335\n",
      "G training 3, loss 0.5630753636360168, D(G(z)) 0.5732928514480591\n",
      "G training 4, loss 0.6425821781158447, D(G(z)) 0.5285241603851318\n",
      "\n",
      "\n",
      "[2/3][10/100]\tLoss_D: 1.4747\tLoss_G: 0.6426\tD(x): 0.5218\tD(G(z)): 0.5476 / 0.5285\n",
      "g_grad:  tensor(2.4208e-05) d_grad tensor(0.0164)\n",
      "G training 1, loss 0.5609204769134521, D(G(z)) 0.5736789703369141\n",
      "G training 2, loss 0.7683308124542236, D(G(z)) 0.47398778796195984\n",
      "G training 3, loss 0.5850246548652649, D(G(z)) 0.5609867572784424\n",
      "G training 4, loss 0.6463375687599182, D(G(z)) 0.5286188125610352\n",
      "\n",
      "\n",
      "[2/3][11/100]\tLoss_D: 1.6175\tLoss_G: 0.6463\tD(x): 0.4597\tD(G(z)): 0.5585 / 0.5286\n",
      "g_grad:  tensor(-6.1117e-05) d_grad tensor(0.0298)\n",
      "G training 1, loss 0.9202433824539185, D(G(z)) 0.41881227493286133\n",
      "G training 2, loss 0.7294083833694458, D(G(z)) 0.48393914103507996\n",
      "G training 3, loss 0.7640295028686523, D(G(z)) 0.4808667004108429\n",
      "G training 4, loss 0.5824626684188843, D(G(z)) 0.5625626444816589\n",
      "\n",
      "\n",
      "[2/3][12/100]\tLoss_D: 2.2174\tLoss_G: 0.5825\tD(x): 0.3370\tD(G(z)): 0.6395 / 0.5626\n",
      "g_grad:  tensor(-9.3890e-06) d_grad tensor(0.0232)\n",
      "G training 1, loss 0.7570598125457764, D(G(z)) 0.4803179204463959\n",
      "G training 2, loss 0.7044705748558044, D(G(z)) 0.5020798444747925\n",
      "G training 3, loss 0.6317085027694702, D(G(z)) 0.5330828428268433\n",
      "G training 4, loss 0.7488451600074768, D(G(z)) 0.4847884178161621\n",
      "\n",
      "\n",
      "[2/3][13/100]\tLoss_D: 1.5816\tLoss_G: 0.7488\tD(x): 0.5218\tD(G(z)): 0.5942 / 0.4848\n",
      "g_grad:  tensor(5.4059e-05) d_grad tensor(0.0106)\n",
      "G training 1, loss 0.7575110197067261, D(G(z)) 0.4800150692462921\n",
      "G training 2, loss 0.867231011390686, D(G(z)) 0.4498355984687805\n",
      "G training 3, loss 0.7019166946411133, D(G(z)) 0.5012754201889038\n",
      "G training 4, loss 0.6180137991905212, D(G(z)) 0.5487417578697205\n",
      "\n",
      "\n",
      "[2/3][14/100]\tLoss_D: 1.1636\tLoss_G: 0.6180\tD(x): 0.5543\tD(G(z)): 0.4151 / 0.5487\n",
      "g_grad:  tensor(-1.3514e-05) d_grad tensor(0.0178)\n",
      "G training 1, loss 0.6649340987205505, D(G(z)) 0.5218960642814636\n",
      "G training 2, loss 0.6267713308334351, D(G(z)) 0.541196882724762\n",
      "G training 3, loss 0.5303742289543152, D(G(z)) 0.5974299907684326\n",
      "G training 4, loss 0.5479232668876648, D(G(z)) 0.5817667245864868\n",
      "\n",
      "\n",
      "[2/3][15/100]\tLoss_D: 2.1035\tLoss_G: 0.5479\tD(x): 0.3157\tD(G(z)): 0.5684 / 0.5818\n",
      "g_grad:  tensor(1.9013e-06) d_grad tensor(0.0147)\n",
      "G training 1, loss 0.6848880648612976, D(G(z)) 0.5066235065460205\n",
      "G training 2, loss 0.6366302371025085, D(G(z)) 0.5429613590240479\n",
      "G training 3, loss 0.44378429651260376, D(G(z)) 0.6419839859008789\n",
      "G training 4, loss 0.6034039258956909, D(G(z)) 0.5505450963973999\n",
      "\n",
      "\n",
      "[2/3][16/100]\tLoss_D: 2.1132\tLoss_G: 0.6034\tD(x): 0.3559\tD(G(z)): 0.5764 / 0.5505\n",
      "g_grad:  tensor(-1.2532e-05) d_grad tensor(0.0101)\n",
      "G training 1, loss 0.8112673759460449, D(G(z)) 0.4459512233734131\n",
      "G training 2, loss 0.7102570533752441, D(G(z)) 0.5049363970756531\n",
      "G training 3, loss 0.4691527783870697, D(G(z)) 0.6286112070083618\n",
      "G training 4, loss 0.6162840127944946, D(G(z)) 0.5420563220977783\n",
      "\n",
      "\n",
      "[2/3][17/100]\tLoss_D: 1.6825\tLoss_G: 0.6163\tD(x): 0.5104\tD(G(z)): 0.6282 / 0.5421\n",
      "g_grad:  tensor(-3.7527e-05) d_grad tensor(0.0094)\n",
      "G training 1, loss 0.7347316145896912, D(G(z)) 0.48401162028312683\n",
      "G training 2, loss 0.7008123397827148, D(G(z)) 0.5023456811904907\n",
      "G training 3, loss 0.6371195912361145, D(G(z)) 0.5347924828529358\n",
      "G training 4, loss 0.5746002793312073, D(G(z)) 0.5660640597343445\n",
      "\n",
      "\n",
      "[2/3][18/100]\tLoss_D: 1.4513\tLoss_G: 0.5746\tD(x): 0.5843\tD(G(z)): 0.5953 / 0.5661\n",
      "g_grad:  tensor(-0.0001) d_grad tensor(0.0103)\n",
      "G training 1, loss 0.8012727499008179, D(G(z)) 0.4554181694984436\n",
      "G training 2, loss 0.6616243720054626, D(G(z)) 0.5166788101196289\n",
      "G training 3, loss 0.6512330174446106, D(G(z)) 0.5220667719841003\n",
      "G training 4, loss 0.5862376689910889, D(G(z)) 0.5576330423355103\n",
      "\n",
      "\n",
      "[2/3][19/100]\tLoss_D: 1.6202\tLoss_G: 0.5862\tD(x): 0.5187\tD(G(z)): 0.6112 / 0.5576\n",
      "g_grad:  tensor(4.3838e-05) d_grad tensor(0.0020)\n",
      "G training 1, loss 0.787781834602356, D(G(z)) 0.4558759033679962\n",
      "G training 2, loss 0.7026457786560059, D(G(z)) 0.4968506097793579\n",
      "G training 3, loss 0.686125636100769, D(G(z)) 0.5090027451515198\n",
      "G training 4, loss 0.6763592958450317, D(G(z)) 0.5119013786315918\n",
      "\n",
      "\n",
      "[2/3][20/100]\tLoss_D: 1.5352\tLoss_G: 0.6764\tD(x): 0.4646\tD(G(z)): 0.5283 / 0.5119\n",
      "g_grad:  tensor(3.0052e-05) d_grad tensor(-0.0029)\n",
      "G training 1, loss 0.7729796171188354, D(G(z)) 0.46460291743278503\n",
      "G training 2, loss 0.805483341217041, D(G(z)) 0.45142221450805664\n",
      "G training 3, loss 0.7418186068534851, D(G(z)) 0.4823608696460724\n",
      "G training 4, loss 0.6557719707489014, D(G(z)) 0.5273218154907227\n",
      "\n",
      "\n",
      "[2/3][21/100]\tLoss_D: 1.6206\tLoss_G: 0.6558\tD(x): 0.4619\tD(G(z)): 0.5515 / 0.5273\n",
      "g_grad:  tensor(-9.2974e-06) d_grad tensor(0.0037)\n",
      "G training 1, loss 0.8311493992805481, D(G(z)) 0.44105634093284607\n",
      "G training 2, loss 0.7106271386146545, D(G(z)) 0.5008453130722046\n",
      "G training 3, loss 0.6273125410079956, D(G(z)) 0.536088228225708\n",
      "G training 4, loss 0.6513966917991638, D(G(z)) 0.5224730968475342\n",
      "\n",
      "\n",
      "[2/3][22/100]\tLoss_D: 2.0648\tLoss_G: 0.6514\tD(x): 0.3039\tD(G(z)): 0.5467 / 0.5225\n",
      "g_grad:  tensor(1.1319e-05) d_grad tensor(-0.0008)\n",
      "G training 1, loss 0.9517134428024292, D(G(z)) 0.3967406153678894\n",
      "G training 2, loss 0.8071237802505493, D(G(z)) 0.47391629219055176\n",
      "G training 3, loss 0.7083514332771301, D(G(z)) 0.4966960847377777\n",
      "G training 4, loss 0.5988500714302063, D(G(z)) 0.5502315163612366\n",
      "\n",
      "\n",
      "[2/3][23/100]\tLoss_D: 1.6156\tLoss_G: 0.5989\tD(x): 0.4697\tD(G(z)): 0.5607 / 0.5502\n",
      "g_grad:  tensor(8.6367e-05) d_grad tensor(-0.0062)\n",
      "G training 1, loss 0.6851264834403992, D(G(z)) 0.5060713887214661\n",
      "G training 2, loss 0.8700496554374695, D(G(z)) 0.4223376512527466\n",
      "G training 3, loss 0.6057116985321045, D(G(z)) 0.5462431907653809\n",
      "G training 4, loss 0.5321468114852905, D(G(z)) 0.5883405804634094\n",
      "\n",
      "\n",
      "[2/3][24/100]\tLoss_D: 1.4464\tLoss_G: 0.5321\tD(x): 0.4821\tD(G(z)): 0.4974 / 0.5883\n",
      "g_grad:  tensor(-6.8791e-05) d_grad tensor(-0.0231)\n",
      "G training 1, loss 0.5517746806144714, D(G(z)) 0.578760027885437\n",
      "G training 2, loss 0.5403693318367004, D(G(z)) 0.5869289636611938\n",
      "G training 3, loss 0.4759761095046997, D(G(z)) 0.6254642605781555\n",
      "G training 4, loss 0.4265299439430237, D(G(z)) 0.6567014455795288\n",
      "\n",
      "\n",
      "[2/3][25/100]\tLoss_D: 1.4378\tLoss_G: 0.4265\tD(x): 0.5382\tD(G(z)): 0.5498 / 0.6567\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(-0.0031)\n",
      "G training 1, loss 0.8263850212097168, D(G(z)) 0.4393773674964905\n",
      "G training 2, loss 0.6978284120559692, D(G(z)) 0.5036864280700684\n",
      "G training 3, loss 0.7668132781982422, D(G(z)) 0.4754590094089508\n",
      "G training 4, loss 0.7289091944694519, D(G(z)) 0.4950469732284546\n",
      "\n",
      "\n",
      "[2/3][26/100]\tLoss_D: 1.9069\tLoss_G: 0.7289\tD(x): 0.5439\tD(G(z)): 0.7141 / 0.4950\n",
      "g_grad:  tensor(-4.0006e-05) d_grad tensor(0.0004)\n",
      "G training 1, loss 0.7195353507995605, D(G(z)) 0.49280571937561035\n",
      "G training 2, loss 0.5999990105628967, D(G(z)) 0.5591591000556946\n",
      "G training 3, loss 0.6084815263748169, D(G(z)) 0.5472339391708374\n",
      "G training 4, loss 0.6212877631187439, D(G(z)) 0.5445452928543091\n",
      "\n",
      "\n",
      "[2/3][27/100]\tLoss_D: 1.6687\tLoss_G: 0.6213\tD(x): 0.3736\tD(G(z)): 0.4709 / 0.5445\n",
      "g_grad:  tensor(-2.5602e-05) d_grad tensor(0.0084)\n",
      "G training 1, loss 0.7303974628448486, D(G(z)) 0.4893718957901001\n",
      "G training 2, loss 0.670623242855072, D(G(z)) 0.5178428888320923\n",
      "G training 3, loss 0.6336869597434998, D(G(z)) 0.5359168648719788\n",
      "G training 4, loss 0.6966854333877563, D(G(z)) 0.5008680820465088\n",
      "\n",
      "\n",
      "[2/3][28/100]\tLoss_D: 1.4042\tLoss_G: 0.6967\tD(x): 0.5386\tD(G(z)): 0.5277 / 0.5009\n",
      "g_grad:  tensor(-3.0068e-05) d_grad tensor(0.0083)\n",
      "G training 1, loss 0.7858495712280273, D(G(z)) 0.4588700830936432\n",
      "G training 2, loss 0.8116624355316162, D(G(z)) 0.44891247153282166\n",
      "G training 3, loss 0.7483471632003784, D(G(z)) 0.47736209630966187\n",
      "G training 4, loss 0.802152156829834, D(G(z)) 0.45630183815956116\n",
      "\n",
      "\n",
      "[2/3][29/100]\tLoss_D: 1.6791\tLoss_G: 0.8022\tD(x): 0.4595\tD(G(z)): 0.5824 / 0.4563\n",
      "g_grad:  tensor(-5.4211e-05) d_grad tensor(0.0178)\n",
      "G training 1, loss 1.1332298517227173, D(G(z)) 0.3335209786891937\n",
      "G training 2, loss 0.9079805016517639, D(G(z)) 0.40649545192718506\n",
      "G training 3, loss 0.7629066705703735, D(G(z)) 0.4737981855869293\n",
      "G training 4, loss 0.8418440818786621, D(G(z)) 0.4319188594818115\n",
      "\n",
      "\n",
      "[2/3][30/100]\tLoss_D: 1.7166\tLoss_G: 0.8418\tD(x): 0.4219\tD(G(z)): 0.5306 / 0.4319\n",
      "g_grad:  tensor(4.7682e-06) d_grad tensor(0.0168)\n",
      "G training 1, loss 0.9015526175498962, D(G(z)) 0.42254507541656494\n",
      "G training 2, loss 0.697141706943512, D(G(z)) 0.4999825060367584\n",
      "G training 3, loss 0.7292880415916443, D(G(z)) 0.4892466366291046\n",
      "G training 4, loss 0.7723802328109741, D(G(z)) 0.46427157521247864\n",
      "\n",
      "\n",
      "[2/3][31/100]\tLoss_D: 1.7394\tLoss_G: 0.7724\tD(x): 0.3551\tD(G(z)): 0.4771 / 0.4643\n",
      "g_grad:  tensor(3.3491e-06) d_grad tensor(0.0049)\n",
      "G training 1, loss 0.6146941184997559, D(G(z)) 0.5412576794624329\n",
      "G training 2, loss 0.7774620652198792, D(G(z)) 0.4705221354961395\n",
      "G training 3, loss 0.6706554889678955, D(G(z)) 0.5189244747161865\n",
      "G training 4, loss 0.5906409621238708, D(G(z)) 0.5558547377586365\n",
      "\n",
      "\n",
      "[2/3][32/100]\tLoss_D: 1.3510\tLoss_G: 0.5906\tD(x): 0.5459\tD(G(z)): 0.5145 / 0.5559\n",
      "g_grad:  tensor(-4.7520e-05) d_grad tensor(0.0105)\n",
      "G training 1, loss 0.6446287631988525, D(G(z)) 0.5291725397109985\n",
      "G training 2, loss 0.7373547554016113, D(G(z)) 0.4833170771598816\n",
      "G training 3, loss 0.6304930448532104, D(G(z)) 0.53421950340271\n",
      "G training 4, loss 0.6033452153205872, D(G(z)) 0.5622659921646118\n",
      "\n",
      "\n",
      "[2/3][33/100]\tLoss_D: 1.5239\tLoss_G: 0.6033\tD(x): 0.5223\tD(G(z)): 0.5727 / 0.5623\n",
      "g_grad:  tensor(-0.0001) d_grad tensor(0.0172)\n",
      "G training 1, loss 0.621805727481842, D(G(z)) 0.5384414792060852\n",
      "G training 2, loss 0.5957795977592468, D(G(z)) 0.554713785648346\n",
      "G training 3, loss 0.5066064596176147, D(G(z)) 0.6065639853477478\n",
      "G training 4, loss 0.507159948348999, D(G(z)) 0.6046837568283081\n",
      "\n",
      "\n",
      "[2/3][34/100]\tLoss_D: 1.7982\tLoss_G: 0.5072\tD(x): 0.5090\tD(G(z)): 0.5615 / 0.6047\n",
      "g_grad:  tensor(7.2309e-05) d_grad tensor(0.0098)\n",
      "G training 1, loss 0.7107986211776733, D(G(z)) 0.49153169989585876\n",
      "G training 2, loss 0.7550290822982788, D(G(z)) 0.4748888611793518\n",
      "G training 3, loss 0.9168354868888855, D(G(z)) 0.4177387058734894\n",
      "G training 4, loss 0.6232547760009766, D(G(z)) 0.5393549203872681\n",
      "\n",
      "\n",
      "[2/3][35/100]\tLoss_D: 1.5129\tLoss_G: 0.6233\tD(x): 0.5288\tD(G(z)): 0.5712 / 0.5394\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0008)\n",
      "G training 1, loss 0.8024056553840637, D(G(z)) 0.4596095383167267\n",
      "G training 2, loss 0.839725911617279, D(G(z)) 0.4400385916233063\n",
      "G training 3, loss 0.6922591924667358, D(G(z)) 0.505980908870697\n",
      "G training 4, loss 0.6217145323753357, D(G(z)) 0.5393728017807007\n",
      "\n",
      "\n",
      "[2/3][36/100]\tLoss_D: 1.4895\tLoss_G: 0.6217\tD(x): 0.6041\tD(G(z)): 0.6174 / 0.5394\n",
      "g_grad:  tensor(3.2497e-05) d_grad tensor(-0.0168)\n",
      "G training 1, loss 0.7781680226325989, D(G(z)) 0.46849721670150757\n",
      "G training 2, loss 0.6721925139427185, D(G(z)) 0.5145595073699951\n",
      "G training 3, loss 0.7325772643089294, D(G(z)) 0.4861660897731781\n",
      "G training 4, loss 0.6604764461517334, D(G(z)) 0.5194981694221497\n",
      "\n",
      "\n",
      "[2/3][37/100]\tLoss_D: 1.4012\tLoss_G: 0.6605\tD(x): 0.4968\tD(G(z)): 0.4909 / 0.5195\n",
      "g_grad:  tensor(-2.3425e-05) d_grad tensor(-0.0043)\n",
      "G training 1, loss 0.8796267509460449, D(G(z)) 0.4170803129673004\n",
      "G training 2, loss 0.699586808681488, D(G(z)) 0.5088566541671753\n",
      "G training 3, loss 0.7248090505599976, D(G(z)) 0.4916820526123047\n",
      "G training 4, loss 0.7056722640991211, D(G(z)) 0.4944848120212555\n",
      "\n",
      "\n",
      "[2/3][38/100]\tLoss_D: 2.1488\tLoss_G: 0.7057\tD(x): 0.3026\tD(G(z)): 0.5698 / 0.4945\n",
      "g_grad:  tensor(2.3309e-05) d_grad tensor(-0.0020)\n",
      "G training 1, loss 0.7756577134132385, D(G(z)) 0.4623969495296478\n",
      "G training 2, loss 0.7747622728347778, D(G(z)) 0.4634346663951874\n",
      "G training 3, loss 0.646974503993988, D(G(z)) 0.5268440842628479\n",
      "G training 4, loss 0.6463706493377686, D(G(z)) 0.5256059765815735\n",
      "\n",
      "\n",
      "[2/3][39/100]\tLoss_D: 1.3814\tLoss_G: 0.6464\tD(x): 0.5408\tD(G(z)): 0.5161 / 0.5256\n",
      "g_grad:  tensor(-2.4351e-05) d_grad tensor(-0.0135)\n",
      "G training 1, loss 0.972490131855011, D(G(z)) 0.3922959268093109\n",
      "G training 2, loss 0.9277651906013489, D(G(z)) 0.41230547428131104\n",
      "G training 3, loss 0.8166356682777405, D(G(z)) 0.4523005187511444\n",
      "G training 4, loss 0.8290282487869263, D(G(z)) 0.44602763652801514\n",
      "\n",
      "\n",
      "[2/3][40/100]\tLoss_D: 1.6217\tLoss_G: 0.8290\tD(x): 0.5023\tD(G(z)): 0.5982 / 0.4460\n",
      "g_grad:  tensor(-9.2438e-05) d_grad tensor(-0.0061)\n",
      "G training 1, loss 0.7455172538757324, D(G(z)) 0.4751206636428833\n",
      "G training 2, loss 0.7828385829925537, D(G(z)) 0.46775442361831665\n",
      "G training 3, loss 0.9888826608657837, D(G(z)) 0.395715594291687\n",
      "G training 4, loss 0.6638079881668091, D(G(z)) 0.5164315104484558\n",
      "\n",
      "\n",
      "[2/3][41/100]\tLoss_D: 1.5987\tLoss_G: 0.6638\tD(x): 0.4281\tD(G(z)): 0.5072 / 0.5164\n",
      "g_grad:  tensor(-4.0737e-06) d_grad tensor(-0.0135)\n",
      "G training 1, loss 0.6573817729949951, D(G(z)) 0.5259131789207458\n",
      "G training 2, loss 0.8159951567649841, D(G(z)) 0.46176767349243164\n",
      "G training 3, loss 0.6213535070419312, D(G(z)) 0.5430145263671875\n",
      "G training 4, loss 0.5470662117004395, D(G(z)) 0.5862929224967957\n",
      "\n",
      "\n",
      "[2/3][42/100]\tLoss_D: 1.4671\tLoss_G: 0.5471\tD(x): 0.4579\tD(G(z)): 0.4778 / 0.5863\n",
      "g_grad:  tensor(-1.4518e-05) d_grad tensor(-0.0047)\n",
      "G training 1, loss 0.8164878487586975, D(G(z)) 0.446174681186676\n",
      "G training 2, loss 0.902807891368866, D(G(z)) 0.42641448974609375\n",
      "G training 3, loss 0.8669082522392273, D(G(z)) 0.44015148282051086\n",
      "G training 4, loss 0.9210996031761169, D(G(z)) 0.4062569737434387\n",
      "\n",
      "\n",
      "[2/3][43/100]\tLoss_D: 1.5897\tLoss_G: 0.9211\tD(x): 0.5265\tD(G(z)): 0.5976 / 0.4063\n",
      "g_grad:  tensor(-3.5370e-05) d_grad tensor(-0.0082)\n",
      "G training 1, loss 1.0258433818817139, D(G(z)) 0.37592369318008423\n",
      "G training 2, loss 0.8482878804206848, D(G(z)) 0.43244409561157227\n",
      "G training 3, loss 0.8252806663513184, D(G(z)) 0.44508668780326843\n",
      "G training 4, loss 0.7170942425727844, D(G(z)) 0.4905325472354889\n",
      "\n",
      "\n",
      "[2/3][44/100]\tLoss_D: 1.4050\tLoss_G: 0.7171\tD(x): 0.5200\tD(G(z)): 0.5153 / 0.4905\n",
      "g_grad:  tensor(8.1687e-05) d_grad tensor(0.0077)\n",
      "G training 1, loss 0.911117434501648, D(G(z)) 0.40984946489334106\n",
      "G training 2, loss 0.8331088423728943, D(G(z)) 0.43709951639175415\n",
      "G training 3, loss 0.7175971269607544, D(G(z)) 0.4905070662498474\n",
      "G training 4, loss 0.7915446162223816, D(G(z)) 0.45481792092323303\n",
      "\n",
      "\n",
      "[2/3][45/100]\tLoss_D: 1.4540\tLoss_G: 0.7915\tD(x): 0.4990\tD(G(z)): 0.5178 / 0.4548\n",
      "g_grad:  tensor(-3.1847e-05) d_grad tensor(0.0159)\n",
      "G training 1, loss 0.8672243356704712, D(G(z)) 0.42399027943611145\n",
      "G training 2, loss 0.8268648982048035, D(G(z)) 0.4462054967880249\n",
      "G training 3, loss 0.5992743372917175, D(G(z)) 0.5535888671875\n",
      "G training 4, loss 0.6477792859077454, D(G(z)) 0.5265384316444397\n",
      "\n",
      "\n",
      "[2/3][46/100]\tLoss_D: 1.2989\tLoss_G: 0.6478\tD(x): 0.5719\tD(G(z)): 0.5153 / 0.5265\n",
      "g_grad:  tensor(-5.1117e-05) d_grad tensor(0.0149)\n",
      "G training 1, loss 0.7806103825569153, D(G(z)) 0.46572625637054443\n",
      "G training 2, loss 0.7282653450965881, D(G(z)) 0.48856571316719055\n",
      "G training 3, loss 0.6642217636108398, D(G(z)) 0.51595538854599\n",
      "G training 4, loss 0.6226634383201599, D(G(z)) 0.546665608882904\n",
      "\n",
      "\n",
      "[2/3][47/100]\tLoss_D: 2.0413\tLoss_G: 0.6227\tD(x): 0.3239\tD(G(z)): 0.5471 / 0.5467\n",
      "g_grad:  tensor(-4.1371e-05) d_grad tensor(0.0017)\n",
      "G training 1, loss 0.9123150110244751, D(G(z)) 0.4061075448989868\n",
      "G training 2, loss 0.7081430554389954, D(G(z)) 0.4969441890716553\n",
      "G training 3, loss 0.842187225818634, D(G(z)) 0.44021084904670715\n",
      "G training 4, loss 0.7463932633399963, D(G(z)) 0.4807847738265991\n",
      "\n",
      "\n",
      "[2/3][48/100]\tLoss_D: 1.4578\tLoss_G: 0.7464\tD(x): 0.5037\tD(G(z)): 0.5123 / 0.4808\n",
      "g_grad:  tensor(7.5805e-05) d_grad tensor(0.0092)\n",
      "G training 1, loss 0.7811030745506287, D(G(z)) 0.4635428488254547\n",
      "G training 2, loss 0.6913834810256958, D(G(z)) 0.501813530921936\n",
      "G training 3, loss 0.6395979523658752, D(G(z)) 0.5294390320777893\n",
      "G training 4, loss 0.5874919891357422, D(G(z)) 0.5574901103973389\n",
      "\n",
      "\n",
      "[2/3][49/100]\tLoss_D: 1.3353\tLoss_G: 0.5875\tD(x): 0.5724\tD(G(z)): 0.5342 / 0.5575\n",
      "g_grad:  tensor(-1.9467e-05) d_grad tensor(0.0016)\n",
      "G training 1, loss 0.7627089619636536, D(G(z)) 0.4683559238910675\n",
      "G training 2, loss 0.803158700466156, D(G(z)) 0.451579213142395\n",
      "G training 3, loss 0.8245986700057983, D(G(z)) 0.44899874925613403\n",
      "G training 4, loss 0.7283474802970886, D(G(z)) 0.48606109619140625\n",
      "\n",
      "\n",
      "[2/3][50/100]\tLoss_D: 1.5102\tLoss_G: 0.7283\tD(x): 0.5604\tD(G(z)): 0.5995 / 0.4861\n",
      "g_grad:  tensor(-5.0472e-05) d_grad tensor(0.0003)\n",
      "G training 1, loss 0.8406742811203003, D(G(z)) 0.4346516728401184\n",
      "G training 2, loss 0.7878146767616272, D(G(z)) 0.4561143219470978\n",
      "G training 3, loss 0.7324415445327759, D(G(z)) 0.4837258756160736\n",
      "G training 4, loss 0.7345006465911865, D(G(z)) 0.4853816032409668\n",
      "\n",
      "\n",
      "[2/3][51/100]\tLoss_D: 1.4001\tLoss_G: 0.7345\tD(x): 0.5257\tD(G(z)): 0.5125 / 0.4854\n",
      "g_grad:  tensor(2.0499e-05) d_grad tensor(0.0028)\n",
      "G training 1, loss 0.7656854391098022, D(G(z)) 0.4653241038322449\n",
      "G training 2, loss 0.7921833395957947, D(G(z)) 0.4564366042613983\n",
      "G training 3, loss 0.6989830732345581, D(G(z)) 0.5040081143379211\n",
      "G training 4, loss 0.872000515460968, D(G(z)) 0.4227712154388428\n",
      "\n",
      "\n",
      "[2/3][52/100]\tLoss_D: 1.3612\tLoss_G: 0.8720\tD(x): 0.5091\tD(G(z)): 0.4870 / 0.4228\n",
      "g_grad:  tensor(2.6851e-05) d_grad tensor(0.0073)\n",
      "G training 1, loss 0.7450594902038574, D(G(z)) 0.48739340901374817\n",
      "G training 2, loss 0.8245190382003784, D(G(z)) 0.44478097558021545\n",
      "G training 3, loss 0.6076527237892151, D(G(z)) 0.5588924884796143\n",
      "G training 4, loss 0.6485787034034729, D(G(z)) 0.5328787565231323\n",
      "\n",
      "\n",
      "[2/3][53/100]\tLoss_D: 1.2603\tLoss_G: 0.6486\tD(x): 0.5360\tD(G(z)): 0.4600 / 0.5329\n",
      "g_grad:  tensor(3.0397e-05) d_grad tensor(0.0090)\n",
      "G training 1, loss 0.9080460071563721, D(G(z)) 0.41036149859428406\n",
      "G training 2, loss 0.7589579224586487, D(G(z)) 0.47992920875549316\n",
      "G training 3, loss 0.726771354675293, D(G(z)) 0.49810877442359924\n",
      "G training 4, loss 0.67807537317276, D(G(z)) 0.517715334892273\n",
      "\n",
      "\n",
      "[2/3][54/100]\tLoss_D: 1.5927\tLoss_G: 0.6781\tD(x): 0.4636\tD(G(z)): 0.5247 / 0.5177\n",
      "g_grad:  tensor(-6.9186e-05) d_grad tensor(0.0067)\n",
      "G training 1, loss 0.7938498854637146, D(G(z)) 0.45464763045310974\n",
      "G training 2, loss 0.8238178491592407, D(G(z)) 0.44356265664100647\n",
      "G training 3, loss 0.783309817314148, D(G(z)) 0.45859768986701965\n",
      "G training 4, loss 0.8147810101509094, D(G(z)) 0.45581844449043274\n",
      "\n",
      "\n",
      "[2/3][55/100]\tLoss_D: 1.7152\tLoss_G: 0.8148\tD(x): 0.5367\tD(G(z)): 0.6353 / 0.4558\n",
      "g_grad:  tensor(-2.1555e-05) d_grad tensor(0.0047)\n",
      "G training 1, loss 0.8287670612335205, D(G(z)) 0.4391288161277771\n",
      "G training 2, loss 0.7416372299194336, D(G(z)) 0.47881579399108887\n",
      "G training 3, loss 0.6809322834014893, D(G(z)) 0.5069275498390198\n",
      "G training 4, loss 0.7565162777900696, D(G(z)) 0.4781532287597656\n",
      "\n",
      "\n",
      "[2/3][56/100]\tLoss_D: 1.7655\tLoss_G: 0.7565\tD(x): 0.3500\tD(G(z)): 0.4887 / 0.4782\n",
      "g_grad:  tensor(-7.8161e-05) d_grad tensor(0.0023)\n",
      "G training 1, loss 0.7906615138053894, D(G(z)) 0.4554726481437683\n",
      "G training 2, loss 0.7316573858261108, D(G(z)) 0.48600539565086365\n",
      "G training 3, loss 0.7721065878868103, D(G(z)) 0.4651772975921631\n",
      "G training 4, loss 0.67645663022995, D(G(z)) 0.5104671120643616\n",
      "\n",
      "\n",
      "[2/3][57/100]\tLoss_D: 1.1797\tLoss_G: 0.6765\tD(x): 0.5620\tD(G(z)): 0.4360 / 0.5105\n",
      "g_grad:  tensor(-8.4876e-05) d_grad tensor(0.0067)\n",
      "G training 1, loss 0.7728829979896545, D(G(z)) 0.47392308712005615\n",
      "G training 2, loss 0.748134970664978, D(G(z)) 0.47990286350250244\n",
      "G training 3, loss 0.6976922154426575, D(G(z)) 0.49866607785224915\n",
      "G training 4, loss 0.6585288643836975, D(G(z)) 0.5219127535820007\n",
      "\n",
      "\n",
      "[2/3][58/100]\tLoss_D: 1.5680\tLoss_G: 0.6585\tD(x): 0.4136\tD(G(z)): 0.4626 / 0.5219\n",
      "g_grad:  tensor(9.2304e-06) d_grad tensor(0.0013)\n",
      "G training 1, loss 0.7312449216842651, D(G(z)) 0.4938284158706665\n",
      "G training 2, loss 0.6970450282096863, D(G(z)) 0.5036972761154175\n",
      "G training 3, loss 0.5693406462669373, D(G(z)) 0.5762555599212646\n",
      "G training 4, loss 0.7566526532173157, D(G(z)) 0.4784187078475952\n",
      "\n",
      "\n",
      "[2/3][59/100]\tLoss_D: 1.5079\tLoss_G: 0.7567\tD(x): 0.4390\tD(G(z)): 0.4495 / 0.4784\n",
      "g_grad:  tensor(-9.0154e-05) d_grad tensor(0.0039)\n",
      "G training 1, loss 0.7014622092247009, D(G(z)) 0.5135030746459961\n",
      "G training 2, loss 0.7246930003166199, D(G(z)) 0.488808274269104\n",
      "G training 3, loss 0.7772918939590454, D(G(z)) 0.4706518054008484\n",
      "G training 4, loss 0.7436617016792297, D(G(z)) 0.48590999841690063\n",
      "\n",
      "\n",
      "[2/3][60/100]\tLoss_D: 1.2301\tLoss_G: 0.7437\tD(x): 0.6109\tD(G(z)): 0.4978 / 0.4859\n",
      "g_grad:  tensor(-6.1375e-06) d_grad tensor(0.0058)\n",
      "G training 1, loss 0.9378060102462769, D(G(z)) 0.4139552712440491\n",
      "G training 2, loss 0.6756536960601807, D(G(z)) 0.512336790561676\n",
      "G training 3, loss 0.7601627111434937, D(G(z)) 0.469063401222229\n",
      "G training 4, loss 0.7234896421432495, D(G(z)) 0.4890938103199005\n",
      "\n",
      "\n",
      "[2/3][61/100]\tLoss_D: 1.2397\tLoss_G: 0.7235\tD(x): 0.6323\tD(G(z)): 0.5334 / 0.4891\n",
      "g_grad:  tensor(-4.2568e-05) d_grad tensor(0.0099)\n",
      "G training 1, loss 0.7635520100593567, D(G(z)) 0.4744783043861389\n",
      "G training 2, loss 0.6254340410232544, D(G(z)) 0.5416227579116821\n",
      "G training 3, loss 0.5903847813606262, D(G(z)) 0.5574155449867249\n",
      "G training 4, loss 0.8109722137451172, D(G(z)) 0.4473721385002136\n",
      "\n",
      "\n",
      "[2/3][62/100]\tLoss_D: 1.5951\tLoss_G: 0.8110\tD(x): 0.4819\tD(G(z)): 0.5364 / 0.4474\n",
      "g_grad:  tensor(-0.0001) d_grad tensor(0.0109)\n",
      "G training 1, loss 0.6891384720802307, D(G(z)) 0.5093880891799927\n",
      "G training 2, loss 0.7523894309997559, D(G(z)) 0.4755173921585083\n",
      "G training 3, loss 0.6791300177574158, D(G(z)) 0.5096608996391296\n",
      "G training 4, loss 0.6698113679885864, D(G(z)) 0.5186871290206909\n",
      "\n",
      "\n",
      "[2/3][63/100]\tLoss_D: 1.5934\tLoss_G: 0.6698\tD(x): 0.5785\tD(G(z)): 0.6142 / 0.5187\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0136)\n",
      "G training 1, loss 0.7666195631027222, D(G(z)) 0.46530935168266296\n",
      "G training 2, loss 0.8910493850708008, D(G(z)) 0.41127777099609375\n",
      "G training 3, loss 0.7109482884407043, D(G(z)) 0.49350544810295105\n",
      "G training 4, loss 0.7819341421127319, D(G(z)) 0.47189438343048096\n",
      "\n",
      "\n",
      "[2/3][64/100]\tLoss_D: 1.4201\tLoss_G: 0.7819\tD(x): 0.5258\tD(G(z)): 0.5341 / 0.4719\n",
      "g_grad:  tensor(-8.6224e-05) d_grad tensor(0.0150)\n",
      "G training 1, loss 0.8142234683036804, D(G(z)) 0.44392064213752747\n",
      "G training 2, loss 0.7090771794319153, D(G(z)) 0.49760371446609497\n",
      "G training 3, loss 0.8718199133872986, D(G(z)) 0.4341033697128296\n",
      "G training 4, loss 0.6892216205596924, D(G(z)) 0.5093640089035034\n",
      "\n",
      "\n",
      "[2/3][65/100]\tLoss_D: 1.8902\tLoss_G: 0.6892\tD(x): 0.3192\tD(G(z)): 0.5173 / 0.5094\n",
      "g_grad:  tensor(-0.0001) d_grad tensor(0.0115)\n",
      "G training 1, loss 0.7731459140777588, D(G(z)) 0.4643794298171997\n",
      "G training 2, loss 0.7367458343505859, D(G(z)) 0.48006781935691833\n",
      "G training 3, loss 0.6307724714279175, D(G(z)) 0.5363178253173828\n",
      "G training 4, loss 0.6640874743461609, D(G(z)) 0.5149757862091064\n",
      "\n",
      "\n",
      "[2/3][66/100]\tLoss_D: 1.8653\tLoss_G: 0.6641\tD(x): 0.3699\tD(G(z)): 0.5400 / 0.5150\n",
      "g_grad:  tensor(5.8845e-05) d_grad tensor(0.0125)\n",
      "G training 1, loss 0.7955747246742249, D(G(z)) 0.45702219009399414\n",
      "G training 2, loss 0.772898256778717, D(G(z)) 0.46883538365364075\n",
      "G training 3, loss 0.5484731793403625, D(G(z)) 0.5799074172973633\n",
      "G training 4, loss 0.6108759641647339, D(G(z)) 0.5465310215950012\n",
      "\n",
      "\n",
      "[2/3][67/100]\tLoss_D: 1.3857\tLoss_G: 0.6109\tD(x): 0.5133\tD(G(z)): 0.5038 / 0.5465\n",
      "g_grad:  tensor(6.0587e-07) d_grad tensor(0.0122)\n",
      "G training 1, loss 0.8599445819854736, D(G(z)) 0.42693692445755005\n",
      "G training 2, loss 0.8269679546356201, D(G(z)) 0.4419570565223694\n",
      "G training 3, loss 0.67231285572052, D(G(z)) 0.5228120684623718\n",
      "G training 4, loss 0.6865747570991516, D(G(z)) 0.5063139200210571\n",
      "\n",
      "\n",
      "[2/3][68/100]\tLoss_D: 1.4677\tLoss_G: 0.6866\tD(x): 0.5041\tD(G(z)): 0.5249 / 0.5063\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0095)\n",
      "G training 1, loss 0.7200846672058105, D(G(z)) 0.48968061804771423\n",
      "G training 2, loss 0.7346749305725098, D(G(z)) 0.4926680326461792\n",
      "G training 3, loss 0.6040799021720886, D(G(z)) 0.5559964776039124\n",
      "G training 4, loss 0.713841438293457, D(G(z)) 0.49343597888946533\n",
      "\n",
      "\n",
      "[2/3][69/100]\tLoss_D: 1.3738\tLoss_G: 0.7138\tD(x): 0.5458\tD(G(z)): 0.5234 / 0.4934\n",
      "g_grad:  tensor(-5.3946e-05) d_grad tensor(0.0108)\n",
      "G training 1, loss 0.6789168119430542, D(G(z)) 0.5216301083564758\n",
      "G training 2, loss 0.5858535766601562, D(G(z)) 0.5697295665740967\n",
      "G training 3, loss 0.573591411113739, D(G(z)) 0.5695313215255737\n",
      "G training 4, loss 0.5280604958534241, D(G(z)) 0.6009260416030884\n",
      "\n",
      "\n",
      "[2/3][70/100]\tLoss_D: 1.8185\tLoss_G: 0.5281\tD(x): 0.3883\tD(G(z)): 0.5686 / 0.6009\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0220)\n",
      "G training 1, loss 0.9281290769577026, D(G(z)) 0.4137327969074249\n",
      "G training 2, loss 1.0226218700408936, D(G(z)) 0.3667793273925781\n",
      "G training 3, loss 1.1575007438659668, D(G(z)) 0.33958661556243896\n",
      "G training 4, loss 0.9604552984237671, D(G(z)) 0.3924228549003601\n",
      "\n",
      "\n",
      "[2/3][71/100]\tLoss_D: 1.6578\tLoss_G: 0.9605\tD(x): 0.5157\tD(G(z)): 0.6152 / 0.3924\n",
      "g_grad:  tensor(-8.1954e-05) d_grad tensor(0.0084)\n",
      "G training 1, loss 0.7630767822265625, D(G(z)) 0.481623113155365\n",
      "G training 2, loss 0.9167295694351196, D(G(z)) 0.41071072220802307\n",
      "G training 3, loss 0.9947839975357056, D(G(z)) 0.3773120045661926\n",
      "G training 4, loss 0.9071998596191406, D(G(z)) 0.4048177897930145\n",
      "\n",
      "\n",
      "[2/3][72/100]\tLoss_D: 1.3263\tLoss_G: 0.9072\tD(x): 0.4449\tD(G(z)): 0.3810 / 0.4048\n",
      "g_grad:  tensor(0.0001) d_grad tensor(0.0079)\n",
      "G training 1, loss 0.7858990430831909, D(G(z)) 0.46493300795555115\n",
      "G training 2, loss 0.7544455528259277, D(G(z)) 0.48329153656959534\n",
      "G training 3, loss 0.8819942474365234, D(G(z)) 0.4181956648826599\n",
      "G training 4, loss 0.7226945161819458, D(G(z)) 0.48960936069488525\n",
      "\n",
      "\n",
      "[2/3][73/100]\tLoss_D: 1.2662\tLoss_G: 0.7227\tD(x): 0.5385\tD(G(z)): 0.4728 / 0.4896\n",
      "g_grad:  tensor(7.4487e-05) d_grad tensor(0.0054)\n",
      "G training 1, loss 0.9177695512771606, D(G(z)) 0.4066867530345917\n",
      "G training 2, loss 0.5533913373947144, D(G(z)) 0.581345796585083\n",
      "G training 3, loss 0.6085470914840698, D(G(z)) 0.5468240976333618\n",
      "G training 4, loss 0.6178051233291626, D(G(z)) 0.5463377833366394\n",
      "\n",
      "\n",
      "[2/3][74/100]\tLoss_D: 1.3850\tLoss_G: 0.6178\tD(x): 0.5018\tD(G(z)): 0.4813 / 0.5463\n",
      "g_grad:  tensor(-2.4746e-05) d_grad tensor(0.0066)\n",
      "G training 1, loss 0.6777656078338623, D(G(z)) 0.5239017605781555\n",
      "G training 2, loss 0.5913864374160767, D(G(z)) 0.5585509538650513\n",
      "G training 3, loss 0.787614643573761, D(G(z)) 0.4606173038482666\n",
      "G training 4, loss 0.6954776048660278, D(G(z)) 0.5022512674331665\n",
      "\n",
      "\n",
      "[2/3][75/100]\tLoss_D: 1.5245\tLoss_G: 0.6955\tD(x): 0.4994\tD(G(z)): 0.5321 / 0.5023\n",
      "g_grad:  tensor(8.8914e-05) d_grad tensor(0.0042)\n",
      "G training 1, loss 1.1162059307098389, D(G(z)) 0.3857005834579468\n",
      "G training 2, loss 0.9388726353645325, D(G(z)) 0.4182048738002777\n",
      "G training 3, loss 0.6469471454620361, D(G(z)) 0.5371286869049072\n",
      "G training 4, loss 0.7623900175094604, D(G(z)) 0.47588837146759033\n",
      "\n",
      "\n",
      "[2/3][76/100]\tLoss_D: 1.3791\tLoss_G: 0.7624\tD(x): 0.6026\tD(G(z)): 0.5752 / 0.4759\n",
      "g_grad:  tensor(5.6220e-05) d_grad tensor(0.0158)\n",
      "G training 1, loss 0.8496618270874023, D(G(z)) 0.44931232929229736\n",
      "G training 2, loss 0.8910185098648071, D(G(z)) 0.42987537384033203\n",
      "G training 3, loss 0.7478740215301514, D(G(z)) 0.4822859466075897\n",
      "G training 4, loss 0.9465627670288086, D(G(z)) 0.39853405952453613\n",
      "\n",
      "\n",
      "[2/3][77/100]\tLoss_D: 1.3856\tLoss_G: 0.9466\tD(x): 0.5914\tD(G(z)): 0.5149 / 0.3985\n",
      "g_grad:  tensor(-9.6253e-05) d_grad tensor(0.0144)\n",
      "G training 1, loss 0.8021599650382996, D(G(z)) 0.4551806151866913\n",
      "G training 2, loss 1.0178602933883667, D(G(z)) 0.3908555209636688\n",
      "G training 3, loss 0.7179076671600342, D(G(z)) 0.4967342019081116\n",
      "G training 4, loss 0.7071235775947571, D(G(z)) 0.5043254494667053\n",
      "\n",
      "\n",
      "[2/3][78/100]\tLoss_D: 1.5019\tLoss_G: 0.7071\tD(x): 0.4348\tD(G(z)): 0.4752 / 0.5043\n",
      "g_grad:  tensor(-0.0004) d_grad tensor(0.0172)\n",
      "G training 1, loss 0.7330822944641113, D(G(z)) 0.4956696033477783\n",
      "G training 2, loss 0.6391697525978088, D(G(z)) 0.5362029075622559\n",
      "G training 3, loss 0.6278765201568604, D(G(z)) 0.5395641326904297\n",
      "G training 4, loss 0.5893272161483765, D(G(z)) 0.5663924813270569\n",
      "\n",
      "\n",
      "[2/3][79/100]\tLoss_D: 1.4390\tLoss_G: 0.5893\tD(x): 0.4722\tD(G(z)): 0.4902 / 0.5664\n",
      "g_grad:  tensor(1.2027e-05) d_grad tensor(0.0110)\n",
      "G training 1, loss 0.8292123675346375, D(G(z)) 0.4438454806804657\n",
      "G training 2, loss 0.7370047569274902, D(G(z)) 0.4840269088745117\n",
      "G training 3, loss 0.7216623425483704, D(G(z)) 0.49432000517845154\n",
      "G training 4, loss 0.6611500978469849, D(G(z)) 0.518691897392273\n",
      "\n",
      "\n",
      "[2/3][80/100]\tLoss_D: 1.5284\tLoss_G: 0.6612\tD(x): 0.4976\tD(G(z)): 0.5199 / 0.5187\n",
      "g_grad:  tensor(0.0003) d_grad tensor(0.0095)\n",
      "G training 1, loss 0.7634041905403137, D(G(z)) 0.47604435682296753\n",
      "G training 2, loss 0.7040146589279175, D(G(z)) 0.5075376033782959\n",
      "G training 3, loss 0.8264275789260864, D(G(z)) 0.4515043795108795\n",
      "G training 4, loss 0.8199362754821777, D(G(z)) 0.45102664828300476\n",
      "\n",
      "\n",
      "[2/3][81/100]\tLoss_D: 1.3126\tLoss_G: 0.8199\tD(x): 0.5605\tD(G(z)): 0.5133 / 0.4510\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0155)\n",
      "G training 1, loss 0.8966966271400452, D(G(z)) 0.4358024001121521\n",
      "G training 2, loss 0.9298465847969055, D(G(z)) 0.4284623861312866\n",
      "G training 3, loss 0.8423288464546204, D(G(z)) 0.45538052916526794\n",
      "G training 4, loss 0.796661913394928, D(G(z)) 0.4567997455596924\n",
      "\n",
      "\n",
      "[2/3][82/100]\tLoss_D: 1.2480\tLoss_G: 0.7967\tD(x): 0.5511\tD(G(z)): 0.4664 / 0.4568\n",
      "g_grad:  tensor(-4.9780e-05) d_grad tensor(0.0249)\n",
      "G training 1, loss 1.2342866659164429, D(G(z)) 0.3084704875946045\n",
      "G training 2, loss 0.9301618337631226, D(G(z)) 0.4100281596183777\n",
      "G training 3, loss 0.678928792476654, D(G(z)) 0.5082772970199585\n",
      "G training 4, loss 0.7781249284744263, D(G(z)) 0.4702654182910919\n",
      "\n",
      "\n",
      "[2/3][83/100]\tLoss_D: 1.3403\tLoss_G: 0.7781\tD(x): 0.5895\tD(G(z)): 0.5413 / 0.4703\n",
      "g_grad:  tensor(-2.9800e-05) d_grad tensor(0.0208)\n",
      "G training 1, loss 0.9342201352119446, D(G(z)) 0.40450727939605713\n",
      "G training 2, loss 0.8204686045646667, D(G(z)) 0.4526905417442322\n",
      "G training 3, loss 0.8568927645683289, D(G(z)) 0.43178316950798035\n",
      "G training 4, loss 0.8780824542045593, D(G(z)) 0.4206044673919678\n",
      "\n",
      "\n",
      "[2/3][84/100]\tLoss_D: 1.4600\tLoss_G: 0.8781\tD(x): 0.4981\tD(G(z)): 0.5235 / 0.4206\n",
      "g_grad:  tensor(-2.1619e-05) d_grad tensor(0.0247)\n",
      "G training 1, loss 0.8599456548690796, D(G(z)) 0.42452025413513184\n",
      "G training 2, loss 0.8298975825309753, D(G(z)) 0.4392307698726654\n",
      "G training 3, loss 0.6913914084434509, D(G(z)) 0.5018263459205627\n",
      "G training 4, loss 0.595874547958374, D(G(z)) 0.5514257550239563\n",
      "\n",
      "\n",
      "[2/3][85/100]\tLoss_D: 1.4222\tLoss_G: 0.5959\tD(x): 0.5158\tD(G(z)): 0.5217 / 0.5514\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0209)\n",
      "G training 1, loss 0.7527213096618652, D(G(z)) 0.4761997163295746\n",
      "G training 2, loss 0.7413944005966187, D(G(z)) 0.4872231185436249\n",
      "G training 3, loss 0.7871103882789612, D(G(z)) 0.47115808725357056\n",
      "G training 4, loss 0.6470053195953369, D(G(z)) 0.5484139323234558\n",
      "\n",
      "\n",
      "[2/3][86/100]\tLoss_D: 1.3299\tLoss_G: 0.6470\tD(x): 0.5209\tD(G(z)): 0.4784 / 0.5484\n",
      "g_grad:  tensor(-2.3500e-07) d_grad tensor(0.0139)\n",
      "G training 1, loss 0.7005499005317688, D(G(z)) 0.4969346821308136\n",
      "G training 2, loss 0.706305742263794, D(G(z)) 0.5045468807220459\n",
      "G training 3, loss 0.7187094688415527, D(G(z)) 0.49328774213790894\n",
      "G training 4, loss 0.6899317502975464, D(G(z)) 0.5034263134002686\n",
      "\n",
      "\n",
      "[2/3][87/100]\tLoss_D: 1.3516\tLoss_G: 0.6899\tD(x): 0.5756\tD(G(z)): 0.5427 / 0.5034\n",
      "g_grad:  tensor(-8.3451e-05) d_grad tensor(0.0104)\n",
      "G training 1, loss 0.8396050930023193, D(G(z)) 0.44486916065216064\n",
      "G training 2, loss 0.960943341255188, D(G(z)) 0.41511064767837524\n",
      "G training 3, loss 0.6926403045654297, D(G(z)) 0.5043127536773682\n",
      "G training 4, loss 0.6360467672348022, D(G(z)) 0.5344057083129883\n",
      "\n",
      "\n",
      "[2/3][88/100]\tLoss_D: 1.1792\tLoss_G: 0.6360\tD(x): 0.5804\tD(G(z)): 0.4595 / 0.5344\n",
      "g_grad:  tensor(0.0002) d_grad tensor(0.0065)\n",
      "G training 1, loss 0.7723453640937805, D(G(z)) 0.47784677147865295\n",
      "G training 2, loss 0.8828049302101135, D(G(z)) 0.45550858974456787\n",
      "G training 3, loss 0.8306612968444824, D(G(z)) 0.46193835139274597\n",
      "G training 4, loss 0.7135690450668335, D(G(z)) 0.4995737373828888\n",
      "\n",
      "\n",
      "[2/3][89/100]\tLoss_D: 1.4190\tLoss_G: 0.7136\tD(x): 0.5238\tD(G(z)): 0.5089 / 0.4996\n",
      "g_grad:  tensor(5.3220e-05) d_grad tensor(-0.0009)\n",
      "G training 1, loss 0.7049027681350708, D(G(z)) 0.49655187129974365\n",
      "G training 2, loss 0.5561250448226929, D(G(z)) 0.5852600932121277\n",
      "G training 3, loss 0.760887086391449, D(G(z)) 0.485542356967926\n",
      "G training 4, loss 0.6414321660995483, D(G(z)) 0.5278838872909546\n",
      "\n",
      "\n",
      "[2/3][90/100]\tLoss_D: 1.4673\tLoss_G: 0.6414\tD(x): 0.5111\tD(G(z)): 0.5181 / 0.5279\n",
      "g_grad:  tensor(3.3006e-05) d_grad tensor(0.0009)\n",
      "G training 1, loss 0.739051878452301, D(G(z)) 0.4886928200721741\n",
      "G training 2, loss 0.666623592376709, D(G(z)) 0.5149552226066589\n",
      "G training 3, loss 0.8335475921630859, D(G(z)) 0.4439719617366791\n",
      "G training 4, loss 0.7127123475074768, D(G(z)) 0.4924861490726471\n",
      "\n",
      "\n",
      "[2/3][91/100]\tLoss_D: 1.3424\tLoss_G: 0.7127\tD(x): 0.6028\tD(G(z)): 0.5549 / 0.4925\n",
      "g_grad:  tensor(0.0008) d_grad tensor(0.0076)\n",
      "G training 1, loss 0.7533262968063354, D(G(z)) 0.47375062108039856\n",
      "G training 2, loss 0.6959702968597412, D(G(z)) 0.5032002329826355\n",
      "G training 3, loss 0.7863613367080688, D(G(z)) 0.4604850113391876\n",
      "G training 4, loss 0.7169229388237, D(G(z)) 0.5029218792915344\n",
      "\n",
      "\n",
      "[2/3][92/100]\tLoss_D: 1.1881\tLoss_G: 0.7169\tD(x): 0.6037\tD(G(z)): 0.4862 / 0.5029\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0293)\n",
      "G training 1, loss 0.8122305274009705, D(G(z)) 0.4497390389442444\n",
      "G training 2, loss 0.6327528357505798, D(G(z)) 0.5390897393226624\n",
      "G training 3, loss 0.7487611770629883, D(G(z)) 0.4816644787788391\n",
      "G training 4, loss 0.7045618891716003, D(G(z)) 0.5017982125282288\n",
      "\n",
      "\n",
      "[2/3][93/100]\tLoss_D: 1.2270\tLoss_G: 0.7046\tD(x): 0.5961\tD(G(z)): 0.4915 / 0.5018\n",
      "g_grad:  tensor(-6.5187e-05) d_grad tensor(0.0145)\n",
      "G training 1, loss 0.7051407694816589, D(G(z)) 0.5206008553504944\n",
      "G training 2, loss 0.7530051469802856, D(G(z)) 0.47228822112083435\n",
      "G training 3, loss 0.6503870487213135, D(G(z)) 0.5337096452713013\n",
      "G training 4, loss 0.7377961874008179, D(G(z)) 0.4876885414123535\n",
      "\n",
      "\n",
      "[2/3][94/100]\tLoss_D: 1.2152\tLoss_G: 0.7378\tD(x): 0.5955\tD(G(z)): 0.4863 / 0.4877\n",
      "g_grad:  tensor(-0.0003) d_grad tensor(0.0079)\n",
      "G training 1, loss 0.7211510539054871, D(G(z)) 0.5179843902587891\n",
      "G training 2, loss 0.6449645161628723, D(G(z)) 0.528988242149353\n",
      "G training 3, loss 0.6785582900047302, D(G(z)) 0.5119816660881042\n",
      "G training 4, loss 0.5750102996826172, D(G(z)) 0.5662508010864258\n",
      "\n",
      "\n",
      "[2/3][95/100]\tLoss_D: 1.5140\tLoss_G: 0.5750\tD(x): 0.4858\tD(G(z)): 0.5183 / 0.5663\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0010)\n",
      "G training 1, loss 0.8583002090454102, D(G(z)) 0.4398319125175476\n",
      "G training 2, loss 0.8050285577774048, D(G(z)) 0.4505825936794281\n",
      "G training 3, loss 0.6787980198860168, D(G(z)) 0.508935809135437\n",
      "G training 4, loss 0.5731417536735535, D(G(z)) 0.5728374123573303\n",
      "\n",
      "\n",
      "[2/3][96/100]\tLoss_D: 1.7708\tLoss_G: 0.5731\tD(x): 0.4532\tD(G(z)): 0.5977 / 0.5728\n",
      "g_grad:  tensor(0.0004) d_grad tensor(-0.0143)\n",
      "G training 1, loss 0.8203801512718201, D(G(z)) 0.44447457790374756\n",
      "G training 2, loss 0.794273853302002, D(G(z)) 0.45800745487213135\n",
      "G training 3, loss 0.6421266794204712, D(G(z)) 0.5398796200752258\n",
      "G training 4, loss 0.8430684804916382, D(G(z)) 0.4392227828502655\n",
      "\n",
      "\n",
      "[2/3][97/100]\tLoss_D: 1.3136\tLoss_G: 0.8431\tD(x): 0.5233\tD(G(z)): 0.4718 / 0.4392\n",
      "g_grad:  tensor(0.0006) d_grad tensor(-0.0086)\n",
      "G training 1, loss 0.6937728524208069, D(G(z)) 0.5115773677825928\n",
      "G training 2, loss 0.7570815086364746, D(G(z)) 0.48504638671875\n",
      "G training 3, loss 1.0139131546020508, D(G(z)) 0.36909225583076477\n",
      "G training 4, loss 0.5971446633338928, D(G(z)) 0.567399263381958\n",
      "\n",
      "\n",
      "[2/3][98/100]\tLoss_D: 1.0861\tLoss_G: 0.5971\tD(x): 0.6301\tD(G(z)): 0.4388 / 0.5674\n",
      "g_grad:  tensor(-0.0002) d_grad tensor(0.0006)\n",
      "G training 1, loss 1.0470916032791138, D(G(z)) 0.38507843017578125\n",
      "G training 2, loss 1.0184708833694458, D(G(z)) 0.3790712356567383\n",
      "G training 3, loss 0.7168862223625183, D(G(z)) 0.5202657580375671\n",
      "G training 4, loss 0.7234388589859009, D(G(z)) 0.5243149995803833\n",
      "\n",
      "\n",
      "[2/3][99/100]\tLoss_D: 1.1786\tLoss_G: 0.7234\tD(x): 0.6285\tD(G(z)): 0.4839 / 0.5243\n",
      "g_grad:  tensor(-0.0005) d_grad tensor(0.0020)\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data.to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu.float()).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        for j in range(1,5):\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            output = netD(fake).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(output, label)\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "            print(\"G training {}, loss {}, D(G(z)) {}\".format(j, errG.item(), D_G_z2))\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 1 == 0:\n",
    "            print('\\n')\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            G_modules_list = list(netG.modules())\n",
    "            D_modules_list = list(netD.modules())\n",
    "            g_param = list(G_modules_list[5].parameters())\n",
    "            d_param = list(D_modules_list[4].parameters())\n",
    "            print(\"g_grad: \", g_param[0].grad[0,0,0,0], \"d_grad\", d_param[0].grad[0,0,0,0])\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25d785630>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(100), G_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2553b9cc0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(300), G_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x251edadd8>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(100), D_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x255456eb8>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(300), D_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_modules_list = list(netG.modules())\n",
    "D_modules_list = list(netD.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G_modules_list[5], D_modules_list[4])\n",
    "g_param = list(G_modules_list[5].parameters())\n",
    "d_param = list(D_modules_list[4].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_param[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_param[0].grad[0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = netG(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = netD(fake).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(S_snapshots[1939,0], cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(step_f(fake[3].detach().cpu()[0]), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(step_f(fake[33].detach().cpu()[0]), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_f = Stepping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1.,  ...,  1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ...,  1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_f(fake[12].detach().cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(data[5][0], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
